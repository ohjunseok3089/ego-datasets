{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for EGOCOM.\n",
    "\n",
    "### This file uses the word_error_rate_analysis package to compute the accuracy of our global transcriptions methods on EGOCOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement # Python 2 compatibility\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# The following package is used to compoute the word error rate (wer).\n",
    "# wer uses the Wagner-Fischer Algorithm to compute the Levenstein distance at both the sentence and word level.\n",
    "# This package requires \"pip install jiwer\"\n",
    "from jiwer import wer \n",
    "\n",
    "# This package converts things like \"42\" to \"forty-two\"\n",
    "from num2words import num2words\n",
    "\n",
    "# For parallel processing\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "max_threads = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from egocom.word_error_rate_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loc = '/media/seagate1tb/egocom-transcription-csv/'\n",
    "ground_truth_csv = '/datasets/cgn/EGOCOM/ground_truth_transcriptions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt = pd.read_csv(ground_truth_csv)\n",
    "df_m1s = [pd.read_csv(csv_loc + \"method_1_speaker_{}.csv\".format(i)) for i in [1,2,3]]\n",
    "df_m2 = pd.read_csv(csv_loc + \"method_2_combined_with_speaker_recognition.csv\")\n",
    "df_m3 = pd.read_csv(csv_loc + \"method_3_ICA.csv\")\n",
    "\n",
    "# Fix columns names (key --> conversation id, speaker --> speaker_id)\n",
    "for i in range(3):\n",
    "    df_m1s[i].columns = [\"conversation_id\", \"startTime\", \"speaker_id\", \"endTime\", \"word\"]\n",
    "df_m2.columns = [\"conversation_id\", \"startTime\", \"speaker_id\", \"endTime\", \"word\"]\n",
    "df_m3.columns = [\"conversation_id\", \"startTime\", \"speaker_id\", \"endTime\", \"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length | 359536\n",
      "After splitting words with spaces into seperate rows | 487746\n",
      "After replacing empty strings with spaces | 487746\n",
      "After removing duplicate rows containing only spaces | 359535\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 359535\n",
      "After 1100 --> one thousand, one hundred | 359535\n",
      "After twenty-two --> twenty two | 359937\n",
      "After removing spaces | 231728\n",
      "After removing capitalization | 231728\n",
      "After removing filler words | 227575\n",
      "CPU times: user 11.9 s, sys: 189 ms, total: 12.1 s\n",
      "Wall time: 6.6 s\n"
     ]
    }
   ],
   "source": [
    "%time gt = create_processed_transcripts(df_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length | 74515\n",
      "After splitting words with spaces into seperate rows | 74515\n",
      "After replacing empty strings with spaces | 74515\n",
      "After removing duplicate rows containing only spaces | 74515\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 82629\n",
      "After 1100 --> one thousand, one hundred | 82629\n",
      "After twenty-two --> twenty two | 82884\n",
      "After removing spaces | 82884\n",
      "After removing capitalization | 82884\n",
      "After removing filler words | 82881\n",
      "Original length | 47412\n",
      "After splitting words with spaces into seperate rows | 47412\n",
      "After replacing empty strings with spaces | 47412\n",
      "After removing duplicate rows containing only spaces | 47412\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 51945\n",
      "After 1100 --> one thousand, one hundred | 51945\n",
      "After twenty-two --> twenty two | 52125\n",
      "After removing spaces | 52125\n",
      "After removing capitalization | 52125\n",
      "After removing filler words | 52123\n",
      "Original length | 38604\n",
      "After splitting words with spaces into seperate rows | 38604\n",
      "After replacing empty strings with spaces | 38604\n",
      "After removing duplicate rows containing only spaces | 38604\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 42347\n",
      "After 1100 --> one thousand, one hundred | 42347\n",
      "After twenty-two --> twenty two | 42454\n",
      "After removing spaces | 42454\n",
      "After removing capitalization | 42454\n",
      "After removing filler words | 42452\n",
      "CPU times: user 3.1 s, sys: 4.03 ms, total: 3.1 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%time m1s = [create_processed_transcripts(df_m1) for df_m1 in df_m1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length | 115557\n",
      "After splitting words with spaces into seperate rows | 115557\n",
      "After replacing empty strings with spaces | 115557\n",
      "After removing duplicate rows containing only spaces | 115557\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 127880\n",
      "After 1100 --> one thousand, one hundred | 127880\n",
      "After twenty-two --> twenty two | 128273\n",
      "After removing spaces | 128273\n",
      "After removing capitalization | 128273\n",
      "After removing filler words | 128267\n",
      "CPU times: user 2.21 s, sys: 23.9 ms, total: 2.23 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%time m2 = create_processed_transcripts(df_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length | 119321\n",
      "After splitting words with spaces into seperate rows | 119321\n",
      "After replacing empty strings with spaces | 119321\n",
      "After removing duplicate rows containing only spaces | 119321\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 132101\n",
      "After 1100 --> one thousand, one hundred | 132101\n",
      "After twenty-two --> twenty two | 132553\n",
      "After removing spaces | 132553\n",
      "After removing capitalization | 132553\n",
      "After removing filler words | 132551\n",
      "CPU times: user 2.26 s, sys: 16.1 ms, total: 2.28 s\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%time m3 = create_processed_transcripts(df_m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both should cover the same set of videos transcribed. m1_3 is fewer (doesnt exist when only two people wore glasses)\n",
    "assert(sorted(gt.keys()) == sorted(m2.keys()))\n",
    "assert(all(sorted(gt.keys()) == sorted(m1.keys()) for m1 in m1s[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "\n",
      "Transcription accuracy for each video\n",
      "-------------------------------------\n",
      "day_1__con_2__part5 | 0.487\n",
      "day_1__con_3__part4 | 0.288\n",
      "day_1__con_1__part5 | 0.392\n",
      "day_1__con_5__part5 | 0.714\n",
      "day_1__con_1__part4 | 0.332\n",
      "day_2__con_1__part1 | 0.451\n",
      "day_1__con_4__part3 | 0.339\n",
      "day_1__con_2__part1 | 0.425\n",
      "day_1__con_3__part1 | 0.36\n",
      "day_1__con_2__part2 | 0.385\n",
      "day_1__con_1__part3 | 0.468\n",
      "day_1__con_5__part1 | 0.412\n",
      "day_1__con_2__part3 | 0.435\n",
      "day_1__con_4__part4 | 0.364\n",
      "day_1__con_1__part1 | 0.362\n",
      "day_1__con_1__part2 | 0.433\n",
      "day_1__con_2__part4 | 0.562\n",
      "day_1__con_4__part2 | 0.365\n",
      "day_1__con_3__part2 | 0.383\n",
      "day_2__con_1__part5 | 0.335\n",
      "day_1__con_4__part1 | 0.487\n",
      "day_1__con_3__part3 | 0.392\n",
      "day_2__con_2__part4 | 0.414\n",
      "day_1__con_5__part4 | 0.393\n",
      "day_2__con_1__part2 | 0.534\n",
      "day_1__con_5__part3 | 0.379\n",
      "day_1__con_5__part2 | 0.391\n",
      "day_2__con_2__part1 | 0.533\n",
      "day_2__con_1__part4 | 0.256\n",
      "day_2__con_1__part3 | 0.326\n",
      "day_2__con_2__part3 | 0.436\n",
      "day_2__con_2__part2 | 0.513\n",
      "day_2__con_3 | 0.437\n",
      "day_3__con_5 | 0.312\n",
      "day_4__con_5 | 0.347\n",
      "day_2__con_6 | 0.412\n",
      "day_4__con_6 | 0.419\n",
      "day_3__con_6 | 0.384\n",
      "day_4__con_3 | 0.449\n",
      "day_3__con_1 | 0.443\n",
      "day_2__con_5 | 0.52\n",
      "day_2__con_7 | 0.484\n",
      "day_3__con_2 | 0.465\n",
      "day_4__con_4 | 0.457\n",
      "day_5__con_2 | 0.508\n",
      "day_5__con_3 | 0.314\n",
      "day_4__con_2 | 0.416\n",
      "day_2__con_4 | 0.472\n",
      "day_3__con_3 | 0.389\n",
      "day_5__con_8 | 0.32\n",
      "day_6__con_4 | 0.332\n",
      "day_5__con_7 | 0.303\n",
      "day_4__con_1 | 0.457\n",
      "day_6__con_5 | 0.465\n",
      "day_3__con_4 | 0.447\n",
      "day_5__con_4 | 0.411\n",
      "day_6__con_3 | 0.354\n",
      "day_5__con_6 | 0.375\n",
      "day_6__con_2 | 0.434\n",
      "day_6__con_6 | 0.323\n",
      "day_6__con_1 | 0.368\n",
      "day_5__con_5 | 0.489\n",
      "day_5__con_1 | 0.424\n",
      "Average Accuracy (1 - word-error-rate): 0.414\n",
      "\n",
      "Transcription accuracy for each video\n",
      "-------------------------------------\n",
      "day_1__con_3__part4 | 0.149\n",
      "day_1__con_2__part5 | 0.287\n",
      "day_1__con_1__part5 | 0.339\n",
      "day_1__con_5__part5 | 0.429\n",
      "day_1__con_1__part3 | 0.241\n",
      "day_1__con_4__part3 | 0.234\n",
      "day_1__con_2__part3 | 0.252\n",
      "day_1__con_3__part2 | 0.233\n",
      "day_1__con_1__part1 | 0.279\n",
      "day_1__con_1__part2 | 0.286\n",
      "day_1__con_3__part3 | 0.192\n",
      "day_1__con_5__part2 | 0.174\n",
      "day_1__con_2__part1 | 0.284\n",
      "day_1__con_4__part2 | 0.184\n",
      "day_1__con_4__part1 | 0.263\n",
      "day_1__con_4__part4 | 0.285\n",
      "day_1__con_5__part1 | 0.287\n",
      "day_1__con_1__part4 | 0.331\n",
      "day_1__con_2__part2 | 0.257\n",
      "day_2__con_1__part1 | 0.442\n",
      "day_1__con_3__part1 | 0.249\n",
      "day_2__con_2__part4 | 0.243\n",
      "day_1__con_5__part3 | 0.251\n",
      "day_2__con_1__part5 | 0.294\n",
      "day_2__con_1__part2 | 0.286\n",
      "day_1__con_5__part4 | 0.23\n",
      "day_2__con_2__part3 | 0.2\n",
      "day_2__con_2__part1 | 0.343\n",
      "day_1__con_2__part4 | 0.504\n",
      "day_2__con_1__part3 | 0.236\n",
      "day_2__con_1__part4 | 0.188\n",
      "day_2__con_2__part2 | 0.233\n",
      "day_2__con_3 | 0.26\n",
      "day_4__con_4 | 0.144\n",
      "day_4__con_3 | 0.201\n",
      "day_2__con_7 | 0.2\n",
      "day_4__con_5 | 0.174\n",
      "day_2__con_5 | 0.23\n",
      "day_3__con_2 | 0.258\n",
      "day_3__con_5 | 0.217\n",
      "day_2__con_6 | 0.239\n",
      "day_4__con_6 | 0.271\n",
      "day_3__con_3 | 0.185\n",
      "day_3__con_6 | 0.283\n",
      "day_3__con_1 | 0.338\n",
      "day_2__con_4 | 0.24\n",
      "day_5__con_4 | 0.149\n",
      "day_5__con_8 | 0.211\n",
      "day_6__con_4 | 0.239\n",
      "day_4__con_1 | 0.236\n",
      "day_5__con_7 | 0.187\n",
      "day_4__con_2 | 0.292\n",
      "day_6__con_5 | 0.258\n",
      "day_5__con_3 | 0.268\n",
      "day_5__con_2 | 0.441\n",
      "day_6__con_2 | 0.239\n",
      "day_6__con_3 | 0.138\n",
      "day_3__con_4 | 0.254\n",
      "day_5__con_5 | 0.24\n",
      "day_5__con_6 | 0.251\n",
      "day_6__con_6 | 0.263\n",
      "day_6__con_1 | 0.306\n",
      "day_5__con_1 | 0.266\n",
      "Average Accuracy (1 - word-error-rate): 0.257\n",
      "\n",
      "Transcription accuracy for each video\n",
      "-------------------------------------\n",
      "day_1__con_2__part5 | 0.252\n",
      "day_1__con_3__part4 | 0.219\n",
      "day_1__con_1__part5 | 0.176\n",
      "day_1__con_2__part2 | 0.188\n",
      "day_2__con_1__part1 | 0.338\n",
      "day_2__con_1__part5 | 0.252\n",
      "day_2__con_1__part2 | 0.31\n",
      "day_1__con_1__part4 | 0.247\n",
      "day_1__con_3__part3 | 0.179\n",
      "day_1__con_1__part2 | 0.219\n",
      "day_1__con_2__part3 | 0.255\n",
      "day_1__con_1__part3 | 0.293\n",
      "day_1__con_2__part1 | 0.259\n",
      "day_1__con_4__part3 | 0.263\n",
      "day_2__con_1__part4 | 0.168\n",
      "day_1__con_3__part1 | 0.251\n",
      "day_1__con_4__part1 | 0.274\n",
      "day_1__con_3__part2 | 0.296\n",
      "day_1__con_4__part4 | 0.259\n",
      "day_1__con_1__part1 | 0.273\n",
      "day_1__con_4__part2 | 0.352\n",
      "day_1__con_2__part4 | 0.41\n",
      "day_2__con_1__part3 | 0.287\n",
      "day_5__con_8 | 0.181\n",
      "day_4__con_3 | 0.207\n",
      "day_4__con_4 | 0.188\n",
      "day_3__con_5 | 0.183\n",
      "day_4__con_5 | 0.203\n",
      "day_4__con_6 | 0.216\n",
      "day_3__con_6 | 0.235\n",
      "day_3__con_2 | 0.225\n",
      "day_5__con_7 | 0.234\n",
      "day_5__con_4 | 0.224\n",
      "day_2__con_6 | 0.266\n",
      "day_2__con_5 | 0.344\n",
      "day_3__con_1 | 0.32\n",
      "day_6__con_4 | 0.151\n",
      "day_2__con_4 | 0.227\n",
      "day_5__con_3 | 0.218\n",
      "day_5__con_6 | 0.289\n",
      "day_3__con_4 | 0.216\n",
      "day_6__con_5 | 0.267\n",
      "day_3__con_3 | 0.261\n",
      "day_6__con_1 | 0.199\n",
      "day_6__con_2 | 0.323\n",
      "day_4__con_1 | 0.339\n",
      "day_5__con_5 | 0.339\n",
      "day_6__con_6 | 0.236\n",
      "day_6__con_3 | 0.272\n",
      "Average Accuracy (1 - word-error-rate): 0.253\n",
      "CPU times: user 517 ms, sys: 634 ms, total: 1.15 s\n",
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "print('Method 1')\n",
    "%time m1_errors = [compute_wer_for_all_videos(m1, gt) for m1 in m1s]\n",
    "# Average error across the 2-3 microphones in each conversation\n",
    "m1_error = {k:np.mean([m1e[k] for m1e in m1_errors if k in m1e]) for k in m1_errors[0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2\n",
      "\n",
      "Transcription accuracy for each video\n",
      "-------------------------------------\n",
      "day_1__con_2__part5 | 0.461\n",
      "day_1__con_3__part4 | 0.486\n",
      "day_1__con_1__part5 | 0.683\n",
      "day_1__con_5__part5 | 0.667\n",
      "day_1__con_4__part3 | 0.445\n",
      "day_2__con_1__part1 | 0.614\n",
      "day_1__con_1__part3 | 0.6\n",
      "day_1__con_2__part1 | 0.592\n",
      "day_1__con_1__part4 | 0.596\n",
      "day_1__con_5__part1 | 0.496\n",
      "day_1__con_2__part2 | 0.559\n",
      "day_1__con_2__part3 | 0.582\n",
      "day_1__con_3__part1 | 0.512\n",
      "day_1__con_4__part4 | 0.525\n",
      "day_1__con_5__part4 | 0.416\n",
      "day_1__con_5__part2 | 0.459\n",
      "day_1__con_4__part2 | 0.588\n",
      "day_1__con_1__part1 | 0.577\n",
      "day_1__con_4__part1 | 0.594\n",
      "day_1__con_1__part2 | 0.633\n",
      "day_1__con_3__part3 | 0.49\n",
      "day_1__con_5__part3 | 0.467\n",
      "day_1__con_2__part4 | 0.664\n",
      "day_2__con_2__part4 | 0.471\n",
      "day_2__con_1__part2 | 0.625\n",
      "day_1__con_3__part2 | 0.527\n",
      "day_2__con_1__part5 | 0.534\n",
      "day_2__con_2__part1 | 0.597\n",
      "day_2__con_2__part3 | 0.469\n",
      "day_2__con_1__part4 | 0.403\n",
      "day_2__con_1__part3 | 0.522\n",
      "day_2__con_2__part2 | 0.55\n",
      "day_2__con_3 | 0.513\n",
      "day_2__con_7 | 0.535\n",
      "day_4__con_3 | 0.494\n",
      "day_4__con_6 | 0.547\n",
      "day_3__con_5 | 0.469\n",
      "day_3__con_1 | 0.597\n",
      "day_4__con_4 | 0.505\n",
      "day_2__con_5 | 0.676\n",
      "day_3__con_6 | 0.526\n",
      "day_2__con_6 | 0.539\n",
      "day_4__con_5 | 0.477\n",
      "day_3__con_2 | 0.601\n",
      "day_5__con_2 | 0.643\n",
      "day_4__con_2 | 0.529\n",
      "day_2__con_4 | 0.548\n",
      "day_6__con_4 | 0.499\n",
      "day_5__con_8 | 0.496\n",
      "day_3__con_3 | 0.566\n",
      "day_5__con_3 | 0.544\n",
      "day_3__con_4 | 0.526\n",
      "day_4__con_1 | 0.6\n",
      "day_6__con_5 | 0.607\n",
      "day_5__con_4 | 0.527\n",
      "day_5__con_7 | 0.486\n",
      "day_6__con_2 | 0.556\n",
      "day_6__con_6 | 0.547\n",
      "day_6__con_3 | 0.513\n",
      "day_5__con_5 | 0.62\n",
      "day_5__con_6 | 0.644\n",
      "day_5__con_1 | 0.507\n",
      "day_6__con_1 | 0.548\n",
      "Average Accuracy (1 - word-error-rate): 0.546\n",
      "CPU times: user 257 ms, sys: 319 ms, total: 576 ms\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "print('Method 2')\n",
    "%time m2_error = compute_wer_for_all_videos(m2, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 3\n",
      "\n",
      "Transcription accuracy for each video\n",
      "-------------------------------------\n",
      "day_1__con_2__part5 | 0.513\n",
      "day_1__con_3__part4 | 0.486\n",
      "day_1__con_1__part5 | 0.661\n",
      "day_1__con_5__part5 | 0.667\n",
      "day_2__con_1__part1 | 0.642\n",
      "day_1__con_2__part1 | 0.621\n",
      "day_1__con_5__part1 | 0.507\n",
      "day_1__con_1__part3 | 0.63\n",
      "day_1__con_4__part3 | 0.484\n",
      "day_1__con_1__part4 | 0.596\n",
      "day_1__con_5__part2 | 0.475\n",
      "day_1__con_3__part1 | 0.531\n",
      "day_1__con_4__part1 | 0.581\n",
      "day_1__con_2__part3 | 0.573\n",
      "day_1__con_5__part4 | 0.396\n",
      "day_1__con_1__part1 | 0.589\n",
      "day_1__con_2__part4 | 0.659\n",
      "day_1__con_2__part2 | 0.564\n",
      "day_1__con_4__part2 | 0.578\n",
      "day_1__con_3__part2 | 0.58\n",
      "day_1__con_3__part3 | 0.509\n",
      "day_1__con_4__part4 | 0.552\n",
      "day_2__con_2__part4 | 0.453\n",
      "day_1__con_1__part2 | 0.623\n",
      "day_2__con_1__part2 | 0.608\n",
      "day_2__con_1__part5 | 0.567\n",
      "day_1__con_5__part3 | 0.472\n",
      "day_2__con_2__part1 | 0.601\n",
      "day_2__con_2__part3 | 0.44\n",
      "day_2__con_1__part3 | 0.557\n",
      "day_2__con_1__part4 | 0.407\n",
      "day_2__con_2__part2 | 0.539\n",
      "day_3__con_2 | 0.177\n",
      "day_2__con_3 | 0.513\n",
      "day_2__con_7 | 0.534\n",
      "day_3__con_1 | 0.611\n",
      "day_3__con_6 | 0.517\n",
      "day_2__con_5 | 0.658\n",
      "day_4__con_3 | 0.421\n",
      "day_2__con_6 | 0.549\n",
      "day_3__con_5 | 0.476\n",
      "day_4__con_5 | 0.485\n",
      "day_4__con_6 | 0.497\n",
      "day_4__con_4 | 0.475\n",
      "day_5__con_2 | 0.639\n",
      "day_4__con_2 | 0.545\n",
      "day_2__con_4 | 0.567\n",
      "day_5__con_8 | 0.409\n",
      "day_5__con_4 | 0.472\n",
      "day_6__con_4 | 0.463\n",
      "day_4__con_1 | 0.62\n",
      "day_3__con_3 | 0.543\n",
      "day_5__con_3 | 0.501\n",
      "day_3__con_4 | 0.534\n",
      "day_6__con_5 | 0.53\n",
      "day_5__con_7 | 0.478\n",
      "day_6__con_2 | 0.562\n",
      "day_6__con_3 | 0.514\n",
      "day_5__con_6 | 0.575\n",
      "day_5__con_5 | 0.473\n",
      "day_6__con_6 | 0.52\n",
      "day_6__con_1 | 0.491\n",
      "day_5__con_1 | 0.431\n",
      "Average Accuracy (1 - word-error-rate): 0.531\n",
      "CPU times: user 297 ms, sys: 298 ms, total: 595 ms\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "print('Method 3')\n",
    "%time m3_error = compute_wer_for_all_videos(m3, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = pd.read_csv(\"/datasets/cgn/EGOCOM/video_info.csv\")\n",
    "video_info['gender'] = video_info['speaker_gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker identification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall speaker id accuracy: 76.76%\n"
     ]
    }
   ],
   "source": [
    "# Get speaker id data\n",
    "gt_ids = df_gt.groupby('conversation_id').apply(lambda x: \" \".join([str(z) for z in x['speaker_id']]))\n",
    "m1_ids = []\n",
    "for i in range(3):\n",
    "    m1_ids.append(df_m1s[i].groupby('conversation_id').apply(lambda x: \" \".join([str(z) for z in x['speaker_id']])))\n",
    "m2_ids = df_m2.groupby('conversation_id').apply(lambda x: \" \".join([str(z) for z in x['speaker_id']]))\n",
    "m3_ids = df_m3.groupby('conversation_id').apply(lambda x: \" \".join([str(z) for z in x['speaker_id']]))\n",
    "\n",
    "# Create dataframes to join when start time is the same and conversation_id is the same.\n",
    "gt_ids = pd.DataFrame(df_gt[['speaker_id', 'startTime', 'conversation_id']].dropna(), copy = True)\n",
    "gt_ids['startTime'] = gt_ids['startTime'].dropna().round().astype(int)\n",
    "m2_ids = pd.DataFrame(df_m2[['speaker_id', 'startTime', 'conversation_id']].dropna(), copy = True)\n",
    "m2_ids['startTime'] = m2_ids['startTime'].dropna().round().astype(int)\n",
    "\n",
    "# Computer accuracy scores.\n",
    "results_id = pd.merge(gt_ids, m2_ids, on=['conversation_id', 'startTime'])\n",
    "id_acc = results_id.groupby('conversation_id').apply(lambda x: sum(x['speaker_id_x'] == x['speaker_id_y']) / len(x))\n",
    "id_df = pd.DataFrame(pd.Series(id_acc), columns = ['speaker_id_acc'])\n",
    "data = video_info.set_index('conversation_id').join(id_df)\n",
    "print('Overall speaker id accuracy:', \"{:.2%}\".format(sum(results_id['speaker_id_x'] == results_id['speaker_id_y']) / len(results_id)))\n",
    "\n",
    "# Compute final dataframes of results\n",
    "results_background = pd.DataFrame(data.groupby(['native_speaker', 'background_music', 'background_fan'])['speaker_id_acc'].mean())\n",
    "results_demographics = pd.DataFrame(data.groupby(['gender', 'native_speaker', 'speaker_is_host'])['speaker_id_acc'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for producing latex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table_header = '''\n",
    "\n",
    "\\\\begin{{table*}}[t]\n",
    "\n",
    "\\\\setlength\\\\tabcolsep{{2pt}} % Makes table columns tighter\n",
    "\\\\caption{{{caption}}}\n",
    "\\\\vskip -0.1in\n",
    "\\\\label{{{label}}}\n",
    "\\\\begin{{center}}\n",
    "\\\\begin{{small}}\n",
    "\\\\begin{{sc}}\n",
    "\\\\resizebox{{1.0\\\\textwidth}}{{!}}{{ %Completely zooms in or zooms out (shrinks) entire table!\n",
    "\n",
    "'''\n",
    "\n",
    "latex_table_footer = '''}}\n",
    "\n",
    "\\\\end{{sc}}\n",
    "\\\\end{{small}}\n",
    "\\\\end{{center}}\n",
    "\\\\vskip -0.1in\n",
    "\\\\end{{table*}}\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex(table, what = 'influencers'):\n",
    "    table = table.reset_index()\n",
    "    table.columns = [z.replace(\"_\", \" \").replace('accuracy', 'acc') for z in list(table.columns)]\n",
    "    caption = \"Global transcription accuracy of baseline vs. our method across {}.\".format(what)\n",
    "    label = \"table:global_transcription_{}\".format(what)\n",
    "    tex = table.to_latex(index = False, column_format = 'ccc|rrrr')\n",
    "    tex_header = latex_table_header.format(\n",
    "        caption = caption, \n",
    "        label = label,\n",
    "    )\n",
    "    tex = tex_header + tex + latex_table_footer    \n",
    "    tex = tex.replace(\"{{\", \"{\")\n",
    "    tex = tex.replace(\"}}\", \"}\")\n",
    "    top, bottom = tex.split('\\\\toprule')\n",
    "    mid, bottom = bottom.split('\\\\midrule')\n",
    "    if what == 'influencers':\n",
    "        mid = '\\n      \\\\textbf{native} & \\\\textbf{music} &  \\\\textbf{fan} &  \\\\textbf{word} &  \\\\textbf{baseline} &  \\\\textbf{egocom} &  \\\\textbf{speaker id} \\\\\\\\\\n \\\\textbf{speaker} &  \\\\textbf{noise} &  \\\\textbf{noise}  &    \\\\textbf{count}  & \\\\textbf{accuracy}   &  \\\\textbf{accuracy}  &  \\\\textbf{accuracy}   \\\\\\\\\\n'\n",
    "    else:\n",
    "        mid = '\\n   &    \\\\textbf{native}  &  \\\\textbf{speaker}  &  \\\\textbf{word}  &  \\\\textbf{baseline} &  \\\\textbf{egocom} &  \\\\textbf{speaker id} \\\\\\\\\\n\\\\textbf{gender} & \\\\textbf{speaker} & \\\\textbf{is host} &    \\\\textbf{count}  & \\\\textbf{accuracy}   &  \\\\textbf{accuracy}  &  \\\\textbf{accuracy}        \\\\\\\\\\n'\n",
    "    bottom = bottom.replace('\\\\\\\\', '\\\\\\\\\\n\\\\hline\\n')\n",
    "    tex = top + '\\\\toprule' + mid + '\\\\midrule' + bottom\n",
    "    tex = tex.replace('True', '\\\\checkmark')\n",
    "    tex = tex.replace('False', '')\n",
    "    tex = tex.replace('\\\\hline\\n\\n\\\\bottomrule', '\\n\\\\bottomrule')\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global transcription accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Acc: 30.67%\n",
      "EgoCom Acc: 54.78%\n",
      "EgoCom speaker id accuracy: 76.76% N =  534499\n"
     ]
    }
   ],
   "source": [
    "baseline_df = pd.DataFrame(pd.Series(m1_error), columns = ['baseline_error'])\n",
    "egocom_df = pd.DataFrame(pd.Series(m2_error), columns = ['egocom_error'])\n",
    "# Combine errors with video data\n",
    "data = video_info.set_index('conversation_id').join(baseline_df).join(egocom_df)\n",
    "data['baseline_error_count'] = (data['word_count'] * data['baseline_error']).astype(int)\n",
    "data['egocom_error_count'] = (data['word_count'] * data['egocom_error']).astype(int)\n",
    "print(\"Baseline Acc:\", \"{:.2%}\".format(1 - data['baseline_error_count'].sum() / data['word_count'].sum()))\n",
    "print(\"EgoCom Acc:\", \"{:.2%}\".format(1 - data['egocom_error_count'].sum() / data['word_count'].sum()))\n",
    "print('EgoCom speaker id accuracy:', \"{:.2%}\".format(sum(results_id['speaker_id_x'] == results_id['speaker_id_y']) / len(results_id)), 'N = ', len(results_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Acc: 0.3045673805331425\n",
      "EgoCom Acc: 0.5448953612514134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>baseline_acc</th>\n",
       "      <th>egocom_acc</th>\n",
       "      <th>speaker_id_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <th>native_speaker</th>\n",
       "      <th>speaker_is_host</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">female</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>1055</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>31666</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">male</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>21174</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>23344</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>81826</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       word_count  baseline_acc  egocom_acc  \\\n",
       "gender native_speaker speaker_is_host                                         \n",
       "female False          False                  1055          0.31        0.54   \n",
       "       True           False                 31666          0.29        0.55   \n",
       "male   False          False                 21174          0.30        0.54   \n",
       "       True           False                 23344          0.31        0.55   \n",
       "                      True                  81826          0.31        0.55   \n",
       "\n",
       "                                       speaker_id_acc  \n",
       "gender native_speaker speaker_is_host                  \n",
       "female False          False                      0.75  \n",
       "       True           False                      0.76  \n",
       "male   False          False                      0.77  \n",
       "       True           False                      0.76  \n",
       "                      True                       0.77  "
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = data.groupby(['gender', 'native_speaker', 'speaker_is_host'])['word_count', 'baseline_error_count', 'egocom_error_count'].sum()\n",
    "results['baseline_acc'] = 1 - results['baseline_error_count'] / results['word_count']\n",
    "results['egocom_acc'] = 1 - results['egocom_error_count'] / results['word_count']\n",
    "print(\"Baseline Acc:\", np.mean(results['baseline_acc']))\n",
    "print(\"EgoCom Acc:\", np.mean(results['egocom_acc']))\n",
    "demo_table = results.join(results_demographics).round(2).drop(['egocom_error_count', 'baseline_error_count'], axis = 1)\n",
    "demo_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Acc: 0.28618166251035765\n",
      "EgoCom Acc: 0.5295406838504789\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "      <th>egocom_accuracy</th>\n",
       "      <th>speaker_id_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>native_speaker</th>\n",
       "      <th>background_music</th>\n",
       "      <th>background_fan</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>17577</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>2467</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>2185</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>96448</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11701</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>28687</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                word_count  baseline_accuracy  \\\n",
       "native_speaker background_music background_fan                                  \n",
       "False          False            False                17577               0.31   \n",
       "                                True                  2467               0.25   \n",
       "               True             False                 2185               0.27   \n",
       "True           False            False                96448               0.32   \n",
       "                                True                 11701               0.28   \n",
       "               True             False                28687               0.29   \n",
       "\n",
       "                                                egocom_accuracy  \\\n",
       "native_speaker background_music background_fan                    \n",
       "False          False            False                      0.55   \n",
       "                                True                       0.51   \n",
       "               True             False                      0.51   \n",
       "True           False            False                      0.56   \n",
       "                                True                       0.53   \n",
       "               True             False                      0.53   \n",
       "\n",
       "                                                speaker_id_acc  \n",
       "native_speaker background_music background_fan                  \n",
       "False          False            False                     0.77  \n",
       "                                True                      0.76  \n",
       "               True             False                     0.79  \n",
       "True           False            False                     0.77  \n",
       "                                True                      0.73  \n",
       "               True             False                     0.76  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = data.groupby(['native_speaker', 'background_music', 'background_fan'])['word_count', 'baseline_error_count', 'egocom_error_count'].sum()\n",
    "results['baseline_accuracy'] = 1 - results['baseline_error_count'] / results['word_count']\n",
    "results['egocom_accuracy'] = 1 - results['egocom_error_count'] / results['word_count']\n",
    "print(\"Baseline Acc:\",np.mean(results['baseline_accuracy']))\n",
    "print(\"EgoCom Acc:\", np.mean(results['egocom_accuracy']))\n",
    "inf_table = results.join(results_background).round(2).drop(['egocom_error_count', 'baseline_error_count'], axis = 1)\n",
    "inf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\\begin{table*}[t]\n",
      "\n",
      "\\setlength\\tabcolsep{2pt} % Makes table columns tighter\n",
      "\\caption{Global transcription accuracy of baseline vs. our method across demographics.}\n",
      "\\vskip -0.1in\n",
      "\\label{table:global_transcription_demographics}\n",
      "\\begin{center}\n",
      "\\begin{small}\n",
      "\\begin{sc}\n",
      "\\resizebox{1.0\\textwidth}{!}{ %Completely zooms in or zooms out (shrinks) entire table!\n",
      "\n",
      "\\begin{tabular}{lll|rrrr}\n",
      "\\toprule\n",
      "      \\textbf{gender}  &    \\textbf{native}  &  \\textbf{speaker}  &  \\textbf{word}  &  \\textbf{baseline} &  \\textbf{egocom} &  \\textbf{speaker id} \\\\\n",
      "\\textbf{gender} & \\textbf{speaker} & \\textbf{is host} &    \\textbf{count}  & \\textbf{accuracy}   &  \\textbf{accuracy}  &  \\textbf{accuracy}        \\\\\n",
      "\\midrule\n",
      " female &            &             &        1055 &          0.31 &        0.54 &            0.75 \\\\\n",
      "\\hline\n",
      "\n",
      " female &            \\checkmark &             &       31666 &          0.29 &        0.55 &            0.76 \\\\\n",
      "\\hline\n",
      "\n",
      "   male &            &             &       21174 &          0.30 &        0.54 &            0.77 \\\\\n",
      "\\hline\n",
      "\n",
      "   male &            \\checkmark &             &       23344 &          0.31 &        0.55 &            0.76 \\\\\n",
      "\\hline\n",
      "\n",
      "   male &            \\checkmark &             \\checkmark &       81826 &          0.31 &        0.55 &            0.77 \\\\\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\n",
      "\\end{sc}\n",
      "\\end{small}\n",
      "\\end{center}\n",
      "\\vskip -0.1in\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{table*}[t]\n",
      "\n",
      "\\setlength\\tabcolsep{2pt} % Makes table columns tighter\n",
      "\\caption{Global transcription accuracy of baseline vs. our method across influencers.}\n",
      "\\vskip -0.1in\n",
      "\\label{table:global_transcription_influencers}\n",
      "\\begin{center}\n",
      "\\begin{small}\n",
      "\\begin{sc}\n",
      "\\resizebox{1.0\\textwidth}{!}{ %Completely zooms in or zooms out (shrinks) entire table!\n",
      "\n",
      "\\begin{tabular}{lll|rrrr}\n",
      "\\toprule\n",
      "      \\textbf{native} & \\textbf{music} &  \\textbf{fan} &  \\textbf{word} &  \\textbf{baseline} &  \\textbf{egocom} &  \\textbf{speaker id} \\\\\n",
      " \\textbf{speaker} &  \\textbf{noise} &  \\textbf{noise}  &    \\textbf{count}  & \\textbf{accuracy}   &  \\textbf{accuracy}  &  \\textbf{accuracy}   \\\\\n",
      "\\midrule\n",
      "           &              &            &       17577 &          0.31 &        0.55 &            0.77 \\\\\n",
      "\\hline\n",
      "\n",
      "           &              &            \\checkmark &        2467 &          0.25 &        0.51 &            0.76 \\\\\n",
      "\\hline\n",
      "\n",
      "           &              \\checkmark &            &        2185 &          0.27 &        0.51 &            0.79 \\\\\n",
      "\\hline\n",
      "\n",
      "           \\checkmark &              &            &       96448 &          0.32 &        0.56 &            0.77 \\\\\n",
      "\\hline\n",
      "\n",
      "           \\checkmark &              &            \\checkmark &       11701 &          0.28 &        0.53 &            0.73 \\\\\n",
      "\\hline\n",
      "\n",
      "           \\checkmark &              \\checkmark &            &       28687 &          0.29 &        0.53 &            0.76 \\\\\n",
      "\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\n",
      "\\end{sc}\n",
      "\\end{small}\n",
      "\\end{center}\n",
      "\\vskip -0.1in\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_latex(demo_table, 'demographics'))\n",
    "print(make_latex(inf_table, 'influencers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_total_error = compute_duration_total_weighted_error(\n",
    "    error_dict = m1_error,\n",
    "    transcript_len_dict = {k:sum([len(m1[k].split()) for m1 in m1s if k in m1]) for k in m1s[0].keys()},\n",
    ")\n",
    "m2_total_error = compute_duration_total_weighted_error(\n",
    "    error_dict = m2_error,\n",
    "    transcript_len_dict = {key: len(s.split()) for key, s in m2.items()},\n",
    ")\n",
    "m3_total_error = compute_duration_total_weighted_error(\n",
    "    error_dict = m3_error,\n",
    "    transcript_len_dict = {key: len(s.split()) for key, s in m3.items()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Method 1 (Single audio transcription, accuracy is avg score across all 3 sources) \n",
      "\tAverage Accuracy: 31.19%\n",
      "\n",
      "Method 2 (Combined transcriptions using source with max confidence for each word)\n",
      "\tAverage Accuracy: 55.04%\n",
      "\n",
      "Method 3 (Combined transcriptions using ICA (directly in time domain) then max-conf-word across sources)\n",
      "\tAverage Accuracy: 52.37%\n"
     ]
    }
   ],
   "source": [
    "print('\\nMethod 1 (Single audio transcription, accuracy is avg score across all 3 sources) \\n\\tAverage Accuracy:', error_as_percent_acc(m1_total_error))\n",
    "print('\\nMethod 2 (Combined transcriptions using source with max confidence for each word)\\n\\tAverage Accuracy:', error_as_percent_acc(m2_total_error))\n",
    "print('\\nMethod 3 (Combined transcriptions using ICA (directly in time domain) then max-conf-word across sources)\\n\\tAverage Accuracy:', error_as_percent_acc(m3_total_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
