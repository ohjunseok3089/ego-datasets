{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script is used to extract the HUMAN GROUND TRUTH TRANSCRIPTIONS from rev.com servers via HTTP FETCH requests and create the EGOCOM/ground_truth_transcriptions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement # Python 2 compatibility\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_str2seconds(x):\n",
    "    if type(x) is str:\n",
    "        return sum(float(x) * 60 ** i for i,x in enumerate(reversed(x.replace(\",\", \".\").split(\":\"))))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_not_include = \"TC0160723531\" # This was a test to make sure the system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orders: 63\n"
     ]
    }
   ],
   "source": [
    "# First, we fetch all the order_ids. Each video transcribed has its own order_id.\n",
    "\n",
    "# curl -X GET   https://www.rev.com/api/v1/orders/TC0160723531 -H 'Authorization: Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='\n",
    "headers = {'Authorization': 'Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='}\n",
    "response = requests.get('https://www.rev.com/api/v1/orders/', headers=headers, params = {'pageSize':100})\n",
    "d = response.json()\n",
    "order_ids = [z['order_number'] for z in d['orders'] if z['order_number'] != do_not_include]\n",
    "print('Number of orders:', len(order_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC0695641954 day_6__con_6.mp4\n",
      "TC0680724528 day_6__con_5.mp4\n",
      "TC0814260226 day_6__con_3.mp4\n",
      "TC0660505040 day_6__con_2.mp4\n",
      "TC0656945670 day_6__con_4.mp4\n",
      "TC0913219738 day_6__con_1.mp4\n",
      "TC1041326357 day_5__con_8.mp4\n",
      "TC0686510667 day_5__con_7.mp4\n",
      "TC1056511204 day_5__con_6.mp4\n",
      "TC0923591085 day_5__con_5.mp4\n",
      "TC0708183445 day_5__con_4.mp4\n",
      "TC0584165371 day_5__con_2.mp4\n",
      "TC0972180451 day_5__con_3.mp4\n",
      "TC0996234767 day_5__con_1.mp4\n",
      "TC0751188622 day_4__con_5.mp4\n",
      "TC0546171684 day_4__con_6.mp4\n",
      "TC0950680326 day_4__con_4.mp4\n",
      "TC0891972543 day_4__con_3.mp4\n",
      "TC0784141698 day_4__con_2.mp4\n",
      "TC0910303778 day_4__con_1.mp4\n",
      "TC0701092819 day_3__con_5.mp4\n",
      "TC0852210878 day_3__con_6.mp4\n",
      "TC0698778753 day_3__con_4.mp4\n",
      "TC0877667951 day_3__con_3.mp4\n",
      "TC0663679227 day_3__con_2.mp4\n",
      "TC0838697727 day_3__con_1.mp4\n",
      "TC0703949877 day_2__con_7.mp4\n",
      "TC0894594926 day_2__con_6.mp4\n",
      "TC0655362525 day_2__con_4.mp4\n",
      "TC0863189618 day_2__con_3.mp4\n",
      "TC0819718827 day_2__con_5.mp4\n",
      "TC0775336185 day_1__con_5__part3.mp4\n",
      "TC1045995822 day_1__con_5__part4.mp4\n",
      "TC0943211261 day_2__con_1__part2.mp4\n",
      "TC0781624395 day_2__con_1__part1.mp4\n",
      "TC0584930529 day_2__con_2__part1.mp4\n",
      "TC0896320790 day_2__con_1__part4.mp4\n",
      "TC0887449316 day_2__con_1__part3.mp4\n",
      "TC0952964890 day_2__con_2__part2.mp4\n",
      "TC0680740787 day_2__con_2__part3.mp4\n",
      "TC0783247646 day_2__con_1__part5.mp4\n",
      "TC1047550391 day_2__con_2__part4.mp4\n",
      "TC0823876816 day_1__con_5__part5.mp4\n",
      "TC0906026344 day_1__con_2__part4.mp4\n",
      "TC0755780993 day_1__con_4__part2.mp4\n",
      "TC0603323958 day_1__con_3__part1.mp4\n",
      "TC0816586629 day_1__con_5__part2.mp4\n",
      "TC1036423226 day_1__con_3__part2.mp4\n",
      "TC0644284755 day_1__con_4__part1.mp4\n",
      "TC0738251607 day_1__con_3__part3.mp4\n",
      "TC1014399884 day_1__con_4__part3.mp4\n",
      "TC0842323440 day_1__con_5__part1.mp4\n",
      "TC0869008937 day_1__con_4__part4.mp4\n",
      "TC0960509791 day_1__con_2__part5.mp4\n",
      "TC0970434548 day_1__con_3__part4.mp4\n",
      "TC0978972799 day_1__con_2__part1.mp4\n",
      "TC0569745556 day_1__con_1__part3.mp4\n",
      "TC0731202147 day_1__con_1__part2.mp4\n",
      "TC0931885720 day_1__con_2__part2.mp4\n",
      "TC0824939623 day_1__con_1__part5.mp4\n",
      "TC0561611932 day_1__con_2__part3.mp4\n",
      "TC0551560810 day_1__con_1__part4.mp4\n",
      "TC0577063922 day_1__con_1__part1.mp4\n"
     ]
    }
   ],
   "source": [
    "json_results = {}\n",
    "for order_id in order_ids:\n",
    "    # Get transcript id.\n",
    "\n",
    "    # curl -X GET   https://www.rev.com/api/v1/orders/TC0160723531 -H 'Authorization: Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='\n",
    "    headers = {'Authorization': 'Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='}\n",
    "    response = requests.get('https://www.rev.com/api/v1/orders/{oid}'.format(oid = order_id), headers=headers)\n",
    "    d = response.json()\n",
    "        \n",
    "    video_name = d['attachments'][0]['name']\n",
    "    \n",
    "    print(order_id, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 320 ms, total: 3.02 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Next, we fetch the raw JSON containing the transcript information for each video.\n",
    "\n",
    "json_results = {}\n",
    "for order_id in order_ids:\n",
    "    # Get transcript id.\n",
    "\n",
    "    # curl -X GET   https://www.rev.com/api/v1/orders/TC0160723531 -H 'Authorization: Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='\n",
    "    headers = {'Authorization': 'Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='}\n",
    "    response = requests.get('https://www.rev.com/api/v1/orders/{oid}'.format(oid = order_id), headers=headers)\n",
    "    d = response.json()\n",
    "    \n",
    "    if len(d['attachments']) <= 1 or 'id' not in d['attachments'][1]:\n",
    "        print('Order', order_id, 'is not ready yet.')\n",
    "        continue\n",
    "    \n",
    "    transcript_id = d['attachments'][1]['id']\n",
    "    video_name = d['attachments'][0]['name']\n",
    "    \n",
    "    # Get transcript data.\n",
    "\n",
    "    # curl -X GET   https://www.rev.com/api/v1/attachments/S3KUCTCGggEAAAAABQAAAA/content -H 'Authorization: Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='\n",
    "    headers = {'Authorization': 'Rev zo2F5eieeq9sWOYRdTHPACuINO4:SdkRAOAPZPHStVkrSO1EcIe67NM='}\n",
    "    response = requests.get('https://www.rev.com/api/v1/attachments/{tid}/content'.format(tid=transcript_id), headers=headers)\n",
    "    json_results[video_name] = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we transform the all JSON data into a Pandas DataFrame organizing all transcriptions.\n",
    "\n",
    "dfs = []\n",
    "for video_name in sorted(json_results.keys()):  \n",
    "    d = json_results[video_name]\n",
    "    [t.update({\"speaker\":sentence[\"speaker\"]}) for sentence in d['monologues'] for t in sentence['elements']]\n",
    "    lod = [z for sent in d['monologues'] for z in sent['elements']]\n",
    "    df = pd.DataFrame(lod)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df[\"key\"] = [video_name[:-4]] * len(df)\n",
    "    df['startTime'] = df['timestamp'].apply(lambda x: convert_time_str2seconds(x)) \n",
    "    df['endTime'] = df['end_timestamp'].apply(lambda x: convert_time_str2seconds(x)) \n",
    "    df['word'] = df['value']\n",
    "    df = df[[\"key\", \"startTime\", \"speaker\", \"endTime\", \"word\"]]\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Remove any speakers included in ground truth beyond the number of actual speakers.\n",
    "# e.g. Sometimes the ground truth (rev.com) includes a speaker for when everyone laughs at once.\n",
    "df = df[~((df[\"key\"] == \"day_2__con_3\") & (df[\"speaker\"] == 2))]\n",
    "df = df[~((df[\"key\"] == \"day_3__con_6\") & (df[\"speaker\"] == 4))]\n",
    "df = df[~((df[\"key\"] == \"day_4__con_2\") & (df[\"speaker\"] == 4))]\n",
    "df = df[~((df[\"key\"] == \"day_4__con_4\") & (df[\"speaker\"] == 2))]\n",
    "df = df[~((df[\"key\"] == \"day_6__con_6\") & (df[\"speaker\"] == 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, sdf in df.groupby('key'):\n",
    "    assert(len(np.unique(sdf['speaker'])) <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part1\n",
      "1 Okay .  So ,  I   have   some   topics   in   my   hand   and   I'll   start   with , \" Name   three   things   that   we   all   have   in   common  \n",
      "2 Hmm . Mm - hmm  (affirmative). Curtis ,  why   didn't   you   wear   pants   today ?  Then   we   could   all   be   wearing   pants . Hmm . ( laughs \n",
      "3 Well ,  none   of   us   hate   the   color   blue . (laughs) The   office   is   always   so   cold ,  though .  Like - ...  I   go   outside   and  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in idmap:\n",
    "    print(key)\n",
    "    for s, sdf in df[df['key'] == key].groupby(\"speaker\"):\n",
    "        print(s, \" \".join(sdf[\"word\"])[:150])\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part1 | day_1__con_1__part2 | day_1__con_1__part3 | day_1__con_1__part4 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgn/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part5 | day_1__con_2__part1 | day_1__con_2__part2 | day_1__con_2__part3 | day_1__con_2__part4 | day_1__con_2__part5 | day_1__con_3__part1 | day_1__con_3__part2 | day_1__con_3__part3 | day_1__con_3__part4 | day_1__con_4__part1 | day_1__con_4__part2 | day_1__con_4__part3 | day_1__con_4__part4 | day_1__con_5__part1 | day_1__con_5__part2 | day_1__con_5__part3 | day_1__con_5__part4 | day_1__con_5__part5 | day_2__con_1__part1 | day_2__con_1__part2 | day_2__con_1__part3 | day_2__con_1__part4 | day_2__con_1__part5 | day_2__con_2__part1 | day_2__con_2__part2 | day_2__con_2__part3 | day_2__con_2__part4 | day_2__con_3 | day_2__con_4 | day_2__con_5 | day_2__con_6 | day_2__con_7 | day_3__con_1 | day_3__con_2 | day_3__con_3 | day_3__con_4 | day_3__con_5 | day_3__con_6 | day_4__con_1 | day_4__con_2 | day_4__con_3 | day_4__con_4 | day_4__con_5 | day_4__con_6 | day_5__con_1 | day_5__con_2 | day_5__con_3 | day_5__con_4 | day_5__con_5 | day_5__con_6 | day_5__con_7 | day_5__con_8 | day_6__con_1 | day_6__con_2 | day_6__con_3 | day_6__con_4 | day_6__con_5 | day_6__con_6 | "
     ]
    }
   ],
   "source": [
    "idmap = {\n",
    "    'day_1__con_1__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_1__part2': {1: 2, 2: 3, 3: 1},\n",
    "    'day_1__con_1__part3': {1: 2, 2: 3, 3: 1},\n",
    "    'day_1__con_1__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_1__part5': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_2__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_2__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_2__part3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_2__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_2__part5': {1: 2, 2: 1, 3: 3},\n",
    "    'day_1__con_3__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_3__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_3__part3': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_3__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_4__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_4__part2': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_4__part3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_4__part4': {1: 3, 2: 2, 3: 1},\n",
    "    'day_1__con_5__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_1__con_5__part4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part5': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_1__part2': {1: 3, 2: 1, 3: 2},\n",
    "    'day_2__con_1__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part4': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_2__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_2__part2': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_2__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_2__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_3': {1: 1, 4: 2, 3: 3},\n",
    "    'day_2__con_4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_6': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_7': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_3__con_3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_3__con_4': {1: 1, 2: 2, 4: 3},\n",
    "    'day_3__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_4__con_3': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_4': {1: 1, 4: 2, 3: 3},\n",
    "    'day_4__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_4__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_1': {1: 3, 2: 2, 3: 1},\n",
    "    'day_5__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_3': {1: 2, 2: 1, 4: 3},\n",
    "    'day_5__con_4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_7': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_8': {1: 2, 2: 3, 3: 1},\n",
    "    'day_6__con_1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_6__con_3': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_6__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_6': {1: 1, 2: 2, 3: 3},\n",
    "}\n",
    "\n",
    "# Map the speaker ids from rev.com to the corect speakers\n",
    "subdfs = []\n",
    "for key, subdf in df.groupby('key'):\n",
    "    print(key, end = \" | \")\n",
    "    subdf[\"speaker\"] = subdf[\"speaker\"].apply(lambda x: np.nan if np.isnan(x) else idmap[key][int(x)])\n",
    "    subdfs.append(subdf)\n",
    "df = pd.concat(subdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>startTime</th>\n",
       "      <th>speaker</th>\n",
       "      <th>endTime</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>day_6__con_6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>Alright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day_6__con_6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>day_6__con_6</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day_6__con_6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_6__con_6</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            key  startTime  speaker  endTime     word\n",
       "0  day_6__con_6       0.00        1     0.28  Alright\n",
       "1  day_6__con_6        NaN        1      NaN       , \n",
       "2  day_6__con_6       0.44        1     0.58     here\n",
       "3  day_6__con_6        NaN        1      NaN         \n",
       "4  day_6__con_6       0.58        1     0.60       we"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['key'] == 'day_6__con_6'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    139030\n",
       "3     64384\n",
       "2     64107\n",
       "Name: speaker, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the multiple tokens on one line\n",
    "# from word_error_rate_analysis import process_transcript_data\n",
    "# df = process_transcript_data(df, remove_actions=False, remove_capitalization=False, remove_filler_words=False, replace_numbers_with_words=False, remove_spaces=False)\n",
    "# df.to_csv(\"/home/cgn/Downloads/egocom-transcription-csv/\" + 'ground_truth_transcriptions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcriptions: 267521\n"
     ]
    }
   ],
   "source": [
    "csv_loc = \"/Users/cgn/Dropbox (Facebook)/EGOCOM/\"\n",
    "# df.to_csv(csv_loc + 'ground_truth_transcriptions.csv', index = False)\n",
    "print('Total transcriptions:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcriptions: 267521\n"
     ]
    }
   ],
   "source": [
    "csv_loc = \"/Users/cgn/Dropbox (Facebook)/EGOCOM/\"\n",
    "df.to_csv(csv_loc + 'ground_truth_transcriptions.csv', index = False)\n",
    "print('Total transcriptions:', len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
