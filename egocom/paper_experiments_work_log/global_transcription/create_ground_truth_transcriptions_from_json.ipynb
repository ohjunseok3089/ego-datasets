{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script is used to extract the HUMAN GROUND TRUTH TRANSCRIPTIONS from rev.com creates the EGOCOM/ground_truth_transcriptions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement # Python 2 compatibility\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_str2seconds(x):\n",
    "    if type(x) is str:\n",
    "        return sum(float(x) * 60 ** i for i,x in enumerate(reversed(x.replace(\",\", \".\").split(\":\"))))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_dir = \"/Users/cgn/Dropbox (Facebook)/EGOCOM/rev_raw_json_transcripts\"\n",
    "json_dir = \"/datasets/cgn/EGOCOM/rev_raw_json_transcripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcript JSON data.\n",
    "json_results = {}\n",
    "for fn in sorted([fn_ for fn_ in os.listdir(json_dir) if \".\" != fn_[0]]):\n",
    "    with open(os.path.join(json_dir, fn), 'r') as f:\n",
    "        transcript_data = f.readlines()\n",
    "        if len(transcript_data) > 1:\n",
    "            print(len(transcript_data))\n",
    "        assert(len(transcript_data)) == 1\n",
    "        json_results[fn[:-5]] = eval(transcript_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all JSON data into a Pandas DataFrame organizing all transcriptions.\n",
    "\n",
    "dfs = []\n",
    "for video_name in sorted(json_results.keys()):  \n",
    "    d = json_results[video_name]\n",
    "    [t.update({\"speaker_id\":sentence[\"speaker\"]}) for sentence in d['monologues'] for t in sentence['elements']]\n",
    "    lod = [z for sent in d['monologues'] for z in sent['elements']]\n",
    "    df = pd.DataFrame(lod)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df[\"conversation_id\"] = [video_name] * len(df)\n",
    "    df['startTime'] = df['timestamp'].apply(lambda x: convert_time_str2seconds(x)) \n",
    "    df['endTime'] = df['end_timestamp'].apply(lambda x: convert_time_str2seconds(x)) \n",
    "    df['word'] = df['value']\n",
    "    df = df[[\"conversation_id\", \"startTime\", \"speaker_id\", \"endTime\", \"word\"]]\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Remove any speakers included in ground truth beyond the number of actual speakers.\n",
    "# e.g. Sometimes the ground truth (rev.com) includes a speaker for when everyone laughs at once.\n",
    "df = df[~((df[\"conversation_id\"] == \"day_2__con_3\") & (df[\"speaker_id\"] == 2))]\n",
    "df = df[~((df[\"conversation_id\"] == \"day_3__con_6\") & (df[\"speaker_id\"] == 4))]\n",
    "df = df[~((df[\"conversation_id\"] == \"day_4__con_2\") & (df[\"speaker_id\"] == 4))]\n",
    "df = df[~((df[\"conversation_id\"] == \"day_4__con_4\") & (df[\"speaker_id\"] == 2))]\n",
    "df = df[~((df[\"conversation_id\"] == \"day_6__con_6\") & (df[\"speaker_id\"] == 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part1 , [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "for d in dfs:\n",
    "    print(d[\"conversation_id\"][1], \",\", d.speaker_id.unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, sdf in df.groupby('conversation_id'):\n",
    "    assert(len(np.unique(sdf['speaker_id'])) <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part1 | day_1__con_1__part2 | day_1__con_1__part3 | day_1__con_1__part4 | day_1__con_1__part5 | day_1__con_2__part1 | day_1__con_2__part2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgn/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | day_1__con_2__part3 | day_1__con_2__part4 | day_1__con_2__part5 | day_1__con_3__part1 | day_1__con_3__part2 | day_1__con_3__part3 | day_1__con_3__part4 | day_1__con_4__part1 | day_1__con_4__part2 | day_1__con_4__part3 | day_1__con_4__part4 | day_1__con_5__part1 | day_1__con_5__part2 | day_1__con_5__part3 | day_1__con_5__part4 | day_1__con_5__part5 | day_2__con_1__part1 | day_2__con_1__part2 | day_2__con_1__part3 | day_2__con_1__part4 | day_2__con_1__part5 | day_2__con_2__part1 | day_2__con_2__part2 | day_2__con_2__part3 | day_2__con_2__part4 | day_2__con_3 | day_2__con_4 | day_2__con_5 | day_2__con_6 | day_2__con_7 | day_3__con_1 | day_3__con_2 | day_3__con_3 | day_3__con_4 | day_3__con_5 | day_3__con_6 | day_4__con_1 | day_4__con_2 | day_4__con_3 | day_4__con_4 | day_4__con_5 | day_4__con_6 | day_5__con_1 | day_5__con_2 | day_5__con_3 | day_5__con_4 | day_5__con_5 | day_5__con_6 | day_5__con_7 | day_5__con_8 | day_6__con_1 | day_6__con_2 | day_6__con_3 | day_6__con_4 | day_6__con_5 | day_6__con_6 | "
     ]
    }
   ],
   "source": [
    "idmap = {\n",
    "    'day_1__con_1__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_1__part2': {1: 2, 2: 3, 3: 1},\n",
    "    'day_1__con_1__part3': {1: 2, 2: 3, 3: 1},\n",
    "    'day_1__con_1__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_1__part5': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_2__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_2__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_2__part3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_2__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_2__part5': {1: 2, 2: 1, 3: 3},\n",
    "    'day_1__con_3__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_3__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_3__part3': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_3__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_1__con_4__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_4__part2': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_4__part3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_1__con_4__part4': {1: 3, 2: 2, 3: 1},\n",
    "    'day_1__con_5__part1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_1__con_5__part4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_1__con_5__part5': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_1__part2': {1: 3, 2: 1, 3: 2},\n",
    "    'day_2__con_1__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part4': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_1__part5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_2__part1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_2__part2': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_2__part3': {1: 2, 2: 1, 3: 3},\n",
    "    'day_2__con_2__part4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_2__con_3': {1: 1, 4: 2, 3: 3},\n",
    "    'day_2__con_4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_6': {1: 1, 2: 3, 3: 2},\n",
    "    'day_2__con_7': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_3__con_3': {1: 3, 2: 1, 3: 2},\n",
    "    'day_3__con_4': {1: 1, 2: 2, 4: 3},\n",
    "    'day_3__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_3__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_1': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_2': {1: 1, 2: 3, 3: 2},\n",
    "    'day_4__con_3': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_4': {1: 1, 4: 2, 3: 3},\n",
    "    'day_4__con_5': {1: 1, 2: 2, 3: 3},\n",
    "    'day_4__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_1': {1: 3, 2: 2, 3: 1},\n",
    "    'day_5__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_3': {1: 2, 2: 1, 4: 3},\n",
    "    'day_5__con_4': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_6': {1: 1, 2: 2, 3: 3},\n",
    "    'day_5__con_7': {1: 1, 2: 3, 3: 2},\n",
    "    'day_5__con_8': {1: 2, 2: 3, 3: 1},\n",
    "    'day_6__con_1': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_2': {1: 1, 2: 2, 3: 3},\n",
    "    'day_6__con_3': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_4': {1: 1, 2: 2, 3: 3},\n",
    "    'day_6__con_5': {1: 1, 2: 3, 3: 2},\n",
    "    'day_6__con_6': {1: 1, 2: 2, 3: 3},\n",
    "}\n",
    "\n",
    "# Map the speaker ids from rev.com to the corect speakers\n",
    "subdfs = []\n",
    "for key, subdf in df.groupby('conversation_id'):\n",
    "    print(key, end = \" | \")\n",
    "    subdf[\"speaker_id\"] = subdf[\"speaker_id\"].apply(lambda x: np.nan if np.isnan(x) else idmap[key][int(x)])\n",
    "    subdfs.append(subdf)\n",
    "df = pd.concat(subdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_1__con_1__part1\n",
      "1 Okay.   So,   I   have   some   topics   in   my   hand   and   I'll   start   with,   \"Name   three   things   that   we   all   have   in   common  \n",
      "2 Well,   none   of   us   hate   the   color   blue. (laughs) The   office   is   always   so   cold,   though.   Like- ...   I   go   outside   and   \n",
      "3 Hmm. Mm-hmm   (affirmative). Curtis,   why   didn't   you   wear   pants   today?   Then   we   could   all   be   wearing   pants. Hmm. (laughs) No? \n",
      "{1: 1, 2: 3, 3: 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the text for each speaker to help created the idmap.\n",
    "for key in idmap:\n",
    "    print(key)\n",
    "    for s, sdf in df[df['conversation_id'] == key].groupby(\"speaker_id\"):\n",
    "        print(s, \" \".join(sdf[\"word\"])[:150])\n",
    "    print(idmap[key])\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    142354\n",
       "3     68108\n",
       "2     66915\n",
       "Name: speaker_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcriptions before post-processing: 277377\n",
      "Original length | 277377\n",
      "After splitting words with spaces into seperate rows | 415913\n",
      "After replacing empty strings with spaces | 415913\n",
      "After removing duplicate rows containing only spaces | 309640\n",
      "After 1900s. --> [1900, s, .] and they've --> [they, ', ve] | 359538\n",
      "Total transcriptions after post-processing: 359538\n"
     ]
    }
   ],
   "source": [
    "# Fix the multiple tokens on one line\n",
    "csv_loc = \"/Users/cgn/Dropbox (Facebook)/EGOCOM/\"\n",
    "csv_loc = \"/datasets/cgn/EGOCOM/\"\n",
    "from egocom.word_error_rate_analysis import process_transcript_data\n",
    "print('Total transcriptions before post-processing:', len(df))\n",
    "df = process_transcript_data(df, remove_capitalization=False, remove_filler_words=False, replace_numbers_with_words=False, remove_spaces=False)\n",
    "print('Total transcriptions after post-processing:', len(df))\n",
    "df.to_csv(csv_loc + 'ground_truth_transcriptions.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
