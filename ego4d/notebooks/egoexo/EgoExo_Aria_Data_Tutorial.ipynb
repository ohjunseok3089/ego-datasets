{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c235c7-7f00-4c3a-8bce-9e8ee2ba138e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a tutorial to better understand what is Project Aria Glasses and its Machine Perception Services (MPS) data linked to an EgoExo scene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc75be-83e3-485a-805c-4576c1ffb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_exo_root = '~/egoexo/' # Replace with your cli's download directory for Ego-Exo4D\n",
    "take_name = 'cmu_bike01_5'\n",
    "\n",
    "import os\n",
    "ego_exo_project_path = os.path.join(ego_exo_root, 'takes', take_name)\n",
    "print(f'EgoExo Sequence: {ego_exo_project_path}')\n",
    "\n",
    "if not os.path.exists(ego_exo_project_path):\n",
    "    print(\"Please do update your path to a valid EgoExo sequence folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c1d0c-4e42-4184-a318-a516abb2943a",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "This tutorial is using Rerun to display temporal and interactive data\n",
    "We are defining here some functions that we will use all across the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca9521-4b19-4b09-87e9-a81e06ccd283",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Visualization utils\n",
    "# Utility function to log to rerun:\n",
    "#  - AriaGlasses outline\n",
    "#  - Camera Calibration\n",
    "#  - Camera pose \n",
    "#  - Images\n",
    "#  - Point cloud\n",
    "##\n",
    "\n",
    "import rerun as rr\n",
    "import numpy as np\n",
    "\n",
    "from projectaria_tools.core.calibration import CameraCalibration, DeviceCalibration\n",
    "from projectaria_tools.core.sophus import SE3\n",
    "\n",
    "def log_aria_glasses(\n",
    "    device_calibration: DeviceCalibration,\n",
    "    label: str,\n",
    "    use_cad_calibration: bool = True\n",
    ") -> None:\n",
    "    ## Plot Project Aria Glasses outline (as lines)\n",
    "    aria_glasses_point_outline = AriaGlassesOutline(\n",
    "        device_calibration, use_cad_calibration\n",
    "    )\n",
    "    rr.log(label, rr.LineStrips3D([aria_glasses_point_outline]), timeless=True)\n",
    "\n",
    "def log_calibration(\n",
    "    camera_calibration: CameraCalibration,\n",
    "    label: str\n",
    ") -> None:\n",
    "    rr.log(\n",
    "        label,\n",
    "        rr.Pinhole(\n",
    "            resolution=[\n",
    "                camera_calibration.get_image_size()[0],\n",
    "                camera_calibration.get_image_size()[1],\n",
    "            ],\n",
    "            focal_length=float(\n",
    "                camera_calibration.get_focal_lengths()[0]\n",
    "            ),\n",
    "        ),\n",
    "        timeless=True,\n",
    "    )\n",
    "\n",
    "def log_pose(\n",
    "    pose: SE3,\n",
    "    label: str,\n",
    "    timeless = False\n",
    ") -> None:\n",
    "    rr.log(\n",
    "        label,\n",
    "        ToTransform3D(pose, False),\n",
    "        timeless = timeless\n",
    "    )\n",
    "\n",
    "def log_image(\n",
    "    image_array : np.array,\n",
    "    label: str,\n",
    "    timeless = False\n",
    ") -> None:\n",
    "    rr.log(label, rr.DisconnectedSpace())\n",
    "    rr.log(label, rr.Image(image_array), timeless = timeless)\n",
    "\n",
    "def log_point_cloud(\n",
    "    point_positions : np.array,\n",
    "    label: str,\n",
    "    timeless: bool = True) -> None:\n",
    "    rr.log(label,rr.Points3D(point_positions, radii=0.001, colors=[200, 200, 200]), timeless=timeless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9736b9-a83c-4a4d-ba63-e73d05213e4d",
   "metadata": {},
   "source": [
    "## Getting to know the Project Aria Glasses and its sensor suite\n",
    "[Project Aria](https://facebookresearch.github.io/projectaria_tools/docs/tech_spec/hardware_spec) is a glasses form factor device to capture multimodal data from an ego-centric perspective.\n",
    "\n",
    "Take Home Message:\n",
    "- You learned about:\n",
    "  - what and where are the various sensors on the Glasses\n",
    "  - That `DeviceCalibration` is the interface to use to retrieve:\n",
    "    - Intrinsics for Image Stream data (i.e Camera data) - `CameraCalibration`\n",
    "    - Extrinsics are defined for all sensors - `SE3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fce2d5-72c0-44b8-af21-a2975c1eacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider\n",
    "from projectaria_tools.utils.rerun_helpers import AriaGlassesOutline, ToTransform3D\n",
    "\n",
    "###\n",
    "# We are using here the projectaria_tools API for:\n",
    "# - retrieving the DeviceCalibration and the POSE of each sensor\n",
    "# - we are then plotting those POSE onto the Aria glasses outline\n",
    "###\n",
    "\n",
    "##\n",
    "# Retrieve device calibration and plot sensors locations, orientations\n",
    "vrs_file_path = os.path.join(ego_exo_project_path, 'aria01.vrs')\n",
    "print(vrs_file_path)\n",
    "assert os.path.exists(vrs_file_path), \"We are not finding the required vrs file\"\n",
    "\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file_path)\n",
    "if not vrs_data_provider:\n",
    "    print(\"Couldn't create data vrs_data_provider from vrs file\")\n",
    "    exit(1)\n",
    "\n",
    "device_calibration = vrs_data_provider.get_device_calibration()\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Aria Glasses\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Aria coordinate system sets X down, Z in front, Y Left\n",
    "rr.log(\"device\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n",
    "\n",
    "cam_labels = device_calibration.get_camera_labels()\n",
    "print(f\"Log {len(cam_labels)} Cameras\")\n",
    "for cam in cam_labels:\n",
    "    camera_calibration = device_calibration.get_camera_calib(cam)\n",
    "    T_device_sensor = camera_calibration.get_transform_device_camera()\n",
    "    log_pose(T_device_sensor, f\"device/camera/{cam}\")\n",
    "    log_calibration(camera_calibration, f\"device/camera/{cam}\")\n",
    "\n",
    "mic_labels = device_calibration.get_microphone_labels()\n",
    "print(f\"Log {len(mic_labels)} Microphones\")\n",
    "for mic in mic_labels:  # Note: Only defined in CAD calibration\n",
    "    T_device_sensor = device_calibration.get_transform_device_sensor(mic, True)\n",
    "    log_pose(T_device_sensor, f\"device/mic/{mic}\")\n",
    "\n",
    "imu_labels = device_calibration.get_imu_labels()\n",
    "print(f\"Log {len(imu_labels)} IMUs\")\n",
    "for imu in imu_labels:\n",
    "    T_device_sensor = device_calibration.get_transform_device_sensor(imu, True)\n",
    "    log_pose(T_device_sensor, f\"device/imu/{imu}\")\n",
    "\n",
    "magnetometer_labels = device_calibration.get_magnetometer_labels()\n",
    "print(f\"Log {len(magnetometer_labels)} Magnetometer\")\n",
    "for magnetometer in magnetometer_labels:  # Note: Only defined in CAD calibration\n",
    "    T_device_sensor = device_calibration.get_transform_device_sensor(magnetometer, True)\n",
    "    log_pose(T_device_sensor, f\"device/{magnetometer}\")\n",
    "\n",
    "barometer_labels = device_calibration.get_barometer_labels()\n",
    "print(f\"Log {len(barometer_labels)} Barometer\")\n",
    "for barometer in barometer_labels:  # Note: Only defined in CAD calibration\n",
    "    T_device_sensor = device_calibration.get_transform_device_sensor(barometer, True)\n",
    "    log_pose(T_device_sensor, f\"device/{barometer}\")\n",
    "    \n",
    "\n",
    "# Plot CPF (Central Pupil Frame coordinate system)\n",
    "T_device_CPF = device_calibration.get_transform_device_cpf()\n",
    "log_pose(T_device_CPF, \"device/CPF_CentralPupilFrame\")\n",
    "\n",
    "# Plot Project Aria Glasses outline (as lines)\n",
    "log_aria_glasses(device_calibration, \"device/glasses_outline\")\n",
    "\n",
    "# Showing the rerun window\n",
    "rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef567d2e-e466-4667-9378-a03e3290d6b0",
   "metadata": {},
   "source": [
    "## What is VRS?\n",
    "Aria is providing VRS files.\n",
    "[VRS](https://facebookresearch.github.io/vrs/) is the file format used to store the Project Aria Glasses multimodal data.\n",
    "\n",
    "We are demonstrating here how to retrieve Image data from a VRS file:\n",
    "- Using the `VrsDataProvider` -> allowing random access\n",
    "- Using the `DeliveryQueue` -> allowing sequential access (as data would come from the glasses if streaming)\n",
    "\n",
    "Take home message:\n",
    "- You learned about:\n",
    "  - VRS Data is stored Stream and are identified with a unique StreamId\n",
    "  - That a convenient class `VrsDataProvider` enables you to list and retrieve all VRS data and calibration data\n",
    "  - That there is a convenient delivery mechanism `DeliveryQueue` to retrieve data as they would be streaming\n",
    "\n",
    "- You now know how to:\n",
    "  - retrieve an data for an arbritraty timestamp\n",
    "     - using `vrs_data_provider.get_X_data_by_time_ns` to retrieve Image or Imu data\n",
    "       - X being `get_image_data_by_time_ns` for Image data\n",
    "       - X being `get_imu_data_by_time_ns` for Imu data\n",
    "    - know how to use the `TimeDomain` option to query `DEVICE_TIME` data (timestamp at which data where captured\n",
    "  - retrieve the configuration of a given stream (i.e Image Size of a Image Stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a95760-b6b7-431d-b6dd-c09035edf3ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# projectaria_tools is providing an easy to use API allowing you to get data for a given StreamId and a given TimeDomain\n",
    "\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from tqdm import tqdm\n",
    "\n",
    "###\n",
    "# We are demonstrating here how to retrieve the time stamp of a given stream\n",
    "# - and how to retrieve 10 frames along this time span\n",
    "###\n",
    "\n",
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "slam_left_stream_id = StreamId(\"1201-1\")\n",
    "slam_right_stream_id = StreamId(\"1201-2\")\n",
    "rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\n",
    "slam_left_stream_label = vrs_data_provider.get_label_from_stream_id(slam_left_stream_id)\n",
    "slam_right_stream_label = vrs_data_provider.get_label_from_stream_id(slam_right_stream_id)\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Aria Data Provider - Retrieve Image Stream data\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Configure option for data retrieval\n",
    "time_domain = TimeDomain.DEVICE_TIME  # query data based on host time\n",
    "option = TimeQueryOptions.CLOSEST # get data whose time [in TimeDomain] is CLOSEST to query time\n",
    "\n",
    "# Retrieve Start and End time for the given Sensor Stream Id\n",
    "start_time = vrs_data_provider.get_first_time_ns(rgb_stream_id, time_domain)\n",
    "end_time = vrs_data_provider.get_last_time_ns(rgb_stream_id, time_domain)\n",
    "\n",
    "# FYI, you can retrieve the Image configuration using the following\n",
    "image_config = vrs_data_provider.get_image_configuration(rgb_stream_id)\n",
    "width = image_config.image_width\n",
    "height = image_config.image_height\n",
    "print(f\"StreamId {rgb_stream_id}, StreamLabel {rgb_stream_label}, ImageSize: {width, height}\")\n",
    "\n",
    "sample_count = 10\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "\n",
    "    # Retrieve the RGB image\n",
    "    image_tuple_rgb = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple_rgb[1].capture_timestamp_ns\n",
    "    \n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    log_image(image_tuple_rgb[0].to_numpy_array(), f\"vrs/{rgb_stream_label}\")\n",
    "\n",
    "    # Retrieving the SLAM images\n",
    "    image_tuple_slam_left = vrs_data_provider.get_image_data_by_time_ns(slam_left_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_left[0].to_numpy_array(), f\"vrs/{slam_left_stream_label}\")\n",
    "\n",
    "    image_tuple_slam_right = vrs_data_provider.get_image_data_by_time_ns(slam_right_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_right[0].to_numpy_array(), f\"vrs/{slam_right_stream_label}\")\n",
    "\n",
    "# Showing the rerun window\n",
    "rec\n",
    "\n",
    "# Note:\n",
    "# See in the Timeline that you have a TIMESTAMP (VRS timestamp) and a Device_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc5b0c-ef1a-45dd-8552-dd5b7019a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Imu data\n",
    "# Showing 'accelerometer readout (m/sec2)' and 'gyroscope readout (rad/sec)'\n",
    "##\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Aria Data Provider - Retrieve IMU data\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "stream_id = vrs_data_provider.get_stream_id_from_label(\"imu-left\")\n",
    "for index in range(0, int(vrs_data_provider.get_num_data(stream_id) / 10)):\n",
    "\n",
    "    imu_data = vrs_data_provider.get_imu_data_by_index(stream_id, index)\n",
    "    timestamp = imu_data.capture_timestamp_ns\n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    rr.log(\"imu-left/accel_msec2/x\", rr.Scalar(imu_data.accel_msec2[0]))\n",
    "    rr.log(\"imu-left/accel_msec2/y\", rr.Scalar(imu_data.accel_msec2[1]))\n",
    "    rr.log(\"imu-left/accel_msec2/z\", rr.Scalar(imu_data.accel_msec2[2]))\n",
    "\n",
    "    rr.log(\"imu-left/gyro_radsec/x\", rr.Scalar(imu_data.gyro_radsec[0]))\n",
    "    rr.log(\"imu-left/gyro_radsec/y\", rr.Scalar(imu_data.gyro_radsec[1]))\n",
    "    rr.log(\"imu-left/gyro_radsec/z\", rr.Scalar(imu_data.gyro_radsec[2]))\n",
    "\n",
    "\n",
    "# Showing the rerun window\n",
    "rec    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816be911-e422-49b2-b7bf-1b631aed60fb",
   "metadata": {},
   "source": [
    "# Using VRS Data Provider and its delivery queue\n",
    "\n",
    "Take Home Message:\n",
    "- You can use the `vrs_data_provider.get_X_data_by_time_ns` to retrieve data chunk at arbritrary timestamp\n",
    "- You can also use a delivery queue to retrieve data as you they were streaming from the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ff259-67ec-4461-8ceb-a695073b169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from projectaria_tools.core.sensor_data import SensorData, SensorDataType\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Aria Data Provider - Delivery Queue\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "# Configure the Delivery Queue for data replay for RGB and EyeTracking images\n",
    "eye_tracking_stream_id = StreamId(\"211-1\")\n",
    "imu_left_stream_id = StreamId(\"1202-2\")\n",
    "\n",
    "deliver_option = vrs_data_provider.get_default_deliver_queued_options()\n",
    "deliver_option.deactivate_stream_all()\n",
    "for stream_id in [rgb_stream_id, eye_tracking_stream_id, imu_left_stream_id]:\n",
    "    deliver_option.activate_stream(stream_id)\n",
    "    deliver_option.set_subsample_rate(stream_id, 60)\n",
    "\n",
    "# Downsampling the image for faster preview\n",
    "down_sampling_factor = 6\n",
    "\n",
    "# Iterate over the data and LOG data as we see fit\n",
    "for data in vrs_data_provider.deliver_queued_sensor_data(deliver_option):\n",
    "    device_time_ns = data.get_time_ns(TimeDomain.DEVICE_TIME)\n",
    "    rr.set_time_nanos(\"device_time\", device_time_ns)\n",
    "    rr.set_time_sequence(\"timestamp\", device_time_ns)\n",
    "\n",
    "    if data.sensor_data_type() == SensorDataType.IMAGE:\n",
    "        img = data.image_data_and_record()[0].to_numpy_array()\n",
    "        img = img[::down_sampling_factor, ::down_sampling_factor]\n",
    "        stream_label = vrs_data_provider.get_label_from_stream_id(data.stream_id())\n",
    "        log_image(img, f\"vrs/{stream_label}\")\n",
    "    elif data.sensor_data_type() == SensorDataType.IMU:\n",
    "        imu_data = data.imu_data()\n",
    "        \n",
    "        rr.log(\"vrs/imu-left/accel_msec2/x\", rr.Scalar(imu_data.accel_msec2[0]))\n",
    "        rr.log(\"vrs/imu-left/accel_msec2/y\", rr.Scalar(imu_data.accel_msec2[1]))\n",
    "        rr.log(\"vrs/imu-left/accel_msec2/z\", rr.Scalar(imu_data.accel_msec2[2]))\n",
    "    \n",
    "        rr.log(\"vrs/imu-left/gyro_radsec/x\", rr.Scalar(imu_data.gyro_radsec[0]))\n",
    "        rr.log(\"vrs/imu-left/gyro_radsec/y\", rr.Scalar(imu_data.gyro_radsec[1]))\n",
    "        rr.log(\"vrs/imu-left/gyro_radsec/z\", rr.Scalar(imu_data.gyro_radsec[2]))\n",
    "\n",
    "# Showing the rerun window\n",
    "rec\n",
    "\n",
    "# NOTE:\n",
    "# Pressing the RELOAD icon next to the \"BluePrint\" keyword on the left, will show the image side by side, and not overlapping!\n",
    "\n",
    "# Do notice on the timeline that the RGB image frequency is HIGHER that the EyeTracking image stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e1a78-3238-4f32-93b9-a98bd399db7e",
   "metadata": {},
   "source": [
    "# Using (MPS) Machine Perception Services Artifacts\n",
    "\n",
    "Each EgoExo scene is coming with metadata such as:\n",
    "- Ego Device trajectory (Aria)\n",
    "- Exo Device camera poses (GoPro)\n",
    "- Point Cloud (3d point cloud + observations)\n",
    "- EyeGaze (where the user is looking at)\n",
    "\n",
    "We are going to learn how to use:\n",
    "\n",
    "- First -> the static assets\n",
    "- Then -> the dynamic assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04905bb5-4a4f-4c47-9513-1acbce31eba0",
   "metadata": {},
   "source": [
    "# Static Assets\n",
    "\n",
    "Take Home Message:\n",
    "- Your learned about load and display static assets, such as:\n",
    "  - How to configure and use `MpsDataProvider`\n",
    "  - Point cloud -> `MpsDataProvider.get_semidense_point_cloud`\n",
    "  - Ego device trajectory -> `mps.read_closed_loop_trajectory`\n",
    "  - Exo devices static GoPros camera -> `mps.read_static_camera_calibrations`\n",
    "\n",
    "\n",
    "- Learn that point cloud data can be noisy and that you can filter out the noisy measurements using user defined thresholds `mps.filter_points_from_confidence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155fb23-0362-4e00-9055-513683ff1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static assets\n",
    "\n",
    "import math\n",
    "from projectaria_tools.core import mps # IO for Aria MPS assets\n",
    "from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3 # Aria/GoPro Camera Calibration\n",
    "\n",
    "from projectaria_tools.core.mps.utils import ( # Aria MPS utilities\n",
    "    filter_points_from_confidence,\n",
    "    filter_points_from_count,\n",
    ")\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"MPS - Static Assets\")\n",
    "rec = rr.memory_recording()\n",
    "rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, timeless=True)\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve Trajectory data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "#\n",
    "## Loading Ego Dynamic device path \n",
    "#\n",
    "print(f\"Loading Device Trajectory: {mps_data_paths.slam.closed_loop_trajectory}\")\n",
    "trajectory_data = mps.read_closed_loop_trajectory(mps_data_paths.slam.closed_loop_trajectory)\n",
    "device_trajectory = [it.transform_world_device.translation()[0] for it in trajectory_data][0::80]\n",
    "\n",
    "rr.log(\"world/device_trajectory\", rr.LineStrips3D(device_trajectory, radii=0.006, colors=[120, 250, 120]), timeless=True)\n",
    "\n",
    "#\n",
    "## Loading and filtering the point cloud\n",
    "#\n",
    "print(\"Loading and filtering point cloud ... be patient ...\")\n",
    "point_cloud = mps_data_provider.get_semidense_point_cloud()\n",
    "# Filter the point cloud by inv depth and depth and load\n",
    "threshold_invdep = 5e-4\n",
    "threshold_dep = 5e-4\n",
    "filtered_point_cloud = filter_points_from_confidence(point_cloud, threshold_invdep, threshold_dep)\n",
    "# Downsampling the data for web viewing\n",
    "downsampled_points_cloud = filter_points_from_count(filtered_point_cloud, 500_000)\n",
    "# Retrieve point positions\n",
    "points_position = np.stack([it.position_world for it in downsampled_points_cloud])\n",
    "\n",
    "log_point_cloud(points_position, \"world/point_cloud\")\n",
    "\n",
    "#\n",
    "## Loading Exo Static camera calibration data\n",
    "#\n",
    "static_calibrations = mps.read_static_camera_calibrations(os.path.join(ego_exo_project_path,\"trajectory\",\"gopro_calibs.csv\"))\n",
    "\n",
    "for static_calibration in static_calibrations:\n",
    "    T_world_device = static_calibration.transform_world_cam\n",
    "    camera_intrinsics = CameraCalibration(static_calibration.camera_uid,\n",
    "                            KANNALA_BRANDT_K3,\n",
    "                            static_calibration.intrinsics,\n",
    "                            static_calibration.transform_world_cam,\n",
    "                            static_calibration.width,\n",
    "                            static_calibration.height,\n",
    "                            None,\n",
    "                            math.pi,\n",
    "                            \"\")\n",
    "\n",
    "    log_calibration(camera_intrinsics, f\"world/{static_calibration.camera_uid}\")\n",
    "    log_pose(T_world_device, f\"world/{static_calibration.camera_uid}\")\n",
    "\n",
    "# Showing the rerun window\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ef5ed-9e2e-4320-8dce-e54d20357523",
   "metadata": {},
   "source": [
    "# Dynamic Assets\n",
    "\n",
    "Take Home Message:\n",
    "- Your learned about load and display dynamic assets, such as:\n",
    "  - Ego device pose at a given timestamp T -> `mps_data_provider.get_closed_loop_pose(timestamp)`\n",
    "  - Ego Eye Gaze ray at a given timestamp T -> `mps_data_provider.get_personalized_eyegaze(timestamp)`\n",
    "\n",
    "- You know how to use relative pose, to display where is the RGB and SLAM cameras on top of the Device pose at time T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d9e995-107d-4e6d-8381-65aeb324071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial on how to use Trajectory data\n",
    "\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core.mps.utils import get_gaze_vector_reprojection\n",
    "\n",
    "###\n",
    "# Trajectory is in Device coordinates\n",
    "# 1. To move to a camera, you have to apply the relative transform from Device_to_Camera_of_your_choice\n",
    "###\n",
    "\n",
    "###\n",
    "# Eye Gaze data is independent of the Device pose\n",
    "# 1. To display the eye gaze ray, you are applying the right relative transform Device_to_CPF\n",
    "###\n",
    "\n",
    "from projectaria_tools.core import mps\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"MPS - Trajectory data\")\n",
    "rec = rr.memory_recording()\n",
    "rr.log(\"world\", rr.ViewCoordinates.RIGHT_HAND_Z_UP, timeless=True)\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve Trajectory data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "assert mps_data_provider.has_personalized_eyegaze(), \"The sequence does not have Eye Gaze data\"\n",
    "assert mps_data_provider.has_closed_loop_poses(), \"The sequence does not have Trajectory data\"\n",
    "\n",
    "# Collect all the trajectory points\n",
    "print(mps_data_paths.slam.closed_loop_trajectory)\n",
    "trajectory_data = mps.read_closed_loop_trajectory(mps_data_paths.slam.closed_loop_trajectory)\n",
    "device_trajectory = [it.transform_world_device.translation()[0] for it in trajectory_data][0::80]\n",
    "\n",
    "rr.log(\"world/device_trajectory\", rr.LineStrips3D(device_trajectory, radii=0.001), timeless=True)\n",
    "\n",
    "# Log Glasses & calibration linked to the image we want to show\n",
    "log_aria_glasses(device_calibration, \"world/device/glasses_outline\")\n",
    "rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "slam_left_camera_calibration = device_calibration.get_camera_calib(slam_left_stream_label)\n",
    "slam_right_camera_calibration = device_calibration.get_camera_calib(slam_right_stream_label)\n",
    "log_calibration(rgb_camera_calibration, f\"world/device/{rgb_stream_label}\")\n",
    "log_calibration(slam_left_camera_calibration, f\"world/device/{slam_left_stream_label}\")\n",
    "log_calibration(slam_right_camera_calibration, f\"world/device/{slam_right_stream_label}\")\n",
    "\n",
    "sample_count = 40\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "    image_tuple = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple[1].capture_timestamp_ns\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    ##\n",
    "    # Retrieve the camera pose at a given timestamp\n",
    "    # 1. Log the Device pose\n",
    "    # 2. Use the extrinsics camera_calibration to apply the relative pose to go from Device to Camera_X\n",
    "    ##\n",
    "    pose_info = mps_data_provider.get_closed_loop_pose(timestamp)\n",
    "    if pose_info:\n",
    "        T_world_device = pose_info.transform_world_device\n",
    "        T_device_camera = rgb_camera_calibration.get_transform_device_camera()\n",
    "\n",
    "        # Log image\n",
    "        log_image(image_tuple[0].to_numpy_array(), f\"world/device/{rgb_stream_label}\")\n",
    "    \n",
    "        # 1. Log the Device pose\n",
    "        log_pose(\n",
    "            T_world_device,\n",
    "            \"world/device\",\n",
    "        )\n",
    "\n",
    "        # 2. Use the extrinsics camera_calibration to apply the relative pose to go from Device to Camera_X\n",
    "        # Show the RGB camera\n",
    "        # Note: Rerun will apply T_world_device @ T_device_camera and display T_world_camera\n",
    "        log_pose(\n",
    "            T_device_camera,\n",
    "            f\"world/device/{rgb_stream_label}\",\n",
    "        )\n",
    "\n",
    "        # Show the SLAM cameras\n",
    "        log_pose(\n",
    "            device_calibration.get_camera_calib(slam_left_stream_label).get_transform_device_camera(),\n",
    "            f\"world/device/{slam_left_stream_label}\",\n",
    "        )\n",
    "        log_pose(\n",
    "            device_calibration.get_camera_calib(slam_right_stream_label).get_transform_device_camera(),\n",
    "            f\"world/device/{slam_left_stream_label}\",\n",
    "        )\n",
    "\n",
    "    ##\n",
    "    # Eye Gaze data\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    ##\n",
    "\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    eye_gaze = mps_data_provider.get_personalized_eyegaze(timestamp)\n",
    "\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # Here is how to retrieve the depth of the EyeGaze vector\n",
    "    # depth_m = eye_gaze.depth or 1.0\n",
    "    # But here for display we are using a proxy of 30cm, so you can better see things in context of each other\n",
    "    depth_m = 0.1\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(eye_gaze.yaw, eye_gaze.pitch, depth_m)\n",
    "    gaze_vector_in_cpf = np.nan_to_num(gaze_vector_in_cpf)\n",
    "    # Move EyeGaze vector to CPF coordinate system for visualization and log a 3D ray\n",
    "    rr.log(\n",
    "        \"world/device/eye-gaze\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_device_CPF @ [0, 0, 0]],\n",
    "            vectors=[T_device_CPF @ gaze_vector_in_cpf],\n",
    "            colors=[[255, 0, 255]],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    depth_m = eye_gaze.depth or 1.0\n",
    "    gaze_projection = get_gaze_vector_reprojection(\n",
    "        eye_gaze,\n",
    "        rgb_stream_label,\n",
    "        device_calibration,\n",
    "        rgb_camera_calibration,\n",
    "        depth_m,\n",
    "    )\n",
    "    if gaze_projection is not None:\n",
    "        rr.log(\n",
    "            f\"world/device/{rgb_stream_label}/eye-gaze_projection\",\n",
    "            rr.Points2D(gaze_projection, radii=30, colors=[0,255,0]),\n",
    "        )\n",
    "\n",
    "# Showing the rerun window\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a16b1-8880-4572-a0c7-848bde9e1706",
   "metadata": {},
   "source": [
    "# Reprojecting Ego data in Exo cameras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395375c0-8cdb-43d8-8ff9-9dc5cea555ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create black images representing GoPro images\n",
    "\n",
    "import math\n",
    "import torchvision # to read video\n",
    "from projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3 # Aria/GoPro Camera Calibration\n",
    "from projectaria_tools.core import mps\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve Trajectory data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Ego_Exo - image reprojection\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "#\n",
    "## Loading Exo Static camera calibration data\n",
    "#\n",
    "go_pro_proxy = []\n",
    "static_calibrations = mps.read_static_camera_calibrations(os.path.join(ego_exo_project_path,\"trajectory\",\"gopro_calibs.csv\"))\n",
    "for static_calibration in static_calibrations:\n",
    "    # assert the GoPro was correctly localized\n",
    "    if static_calibration.quality != 1.0:\n",
    "        print(f\"Camera: {static_calibration.camera_uid} was not localized, ignoring this camera.\")\n",
    "        continue\n",
    "    proxy = {}\n",
    "    proxy[\"name\"] = static_calibration.camera_uid\n",
    "    proxy[\"image\"] = zeros = np.zeros((static_calibration.height, static_calibration.width))\n",
    "    proxy[\"pose\"] = static_calibration.transform_world_cam\n",
    "    proxy[\"camera\"] = CameraCalibration(\n",
    "                            static_calibration.camera_uid,\n",
    "                            KANNALA_BRANDT_K3,\n",
    "                            static_calibration.intrinsics,\n",
    "                            static_calibration.transform_world_cam,\n",
    "                            static_calibration.width,\n",
    "                            static_calibration.height,\n",
    "                            None,\n",
    "                            math.pi,\n",
    "                            \"\")\n",
    "\n",
    "    # Replace proxy image with an image from the gopro video\n",
    "    video_path = os.path.join( ego_exo_project_path ,\"frame_aligned_videos\", static_calibration.camera_uid + \".mp4\")\n",
    "    reader = torchvision.io.VideoReader(video_path, \"video\")\n",
    "    # Grab a frame at the middle of the video\n",
    "    reader_metadata = reader.get_metadata()\n",
    "    reader.seek(reader_metadata['video']['duration'][0] / 2)\n",
    "    frame = next(reader)\n",
    "    proxy[\"image\"] = frame['data'][0].numpy()\n",
    "    \n",
    "    # Log the image\n",
    "    log_image(proxy[\"image\"], f\"image/{proxy['name']}\", timeless= True)\n",
    "\n",
    "    go_pro_proxy.append(proxy)\n",
    "\n",
    "\n",
    "per_go_pro_reprojection = {}\n",
    "# Sample the camera trajectory and reproject it on the GoPro images\n",
    "\n",
    "sample_count = 120\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "    image_tuple = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple[1].capture_timestamp_ns\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    ##\n",
    "    # Retrieve the camera pose at a given timestamp\n",
    "    # 1. Log the Device pose\n",
    "    # 2. Use the extrinsics camera_calibration to apply the relative pose to go from Device to Camera_X\n",
    "    ##\n",
    "    pose_info = mps_data_provider.get_closed_loop_pose(timestamp)\n",
    "    if pose_info:\n",
    "        T_world_device = pose_info.transform_world_device\n",
    "        for go_pro in go_pro_proxy:\n",
    "            point_in_go_pro_world = go_pro[\"pose\"].inverse() @ T_world_device.translation()[0]\n",
    "            device_projection = go_pro[\"camera\"].project(point_in_go_pro_world)\n",
    "            if device_projection is not None:\n",
    "                if go_pro['name'] not in per_go_pro_reprojection.keys():\n",
    "                    per_go_pro_reprojection[go_pro['name']] = []\n",
    "                per_go_pro_reprojection[go_pro['name']].append(device_projection)\n",
    "\n",
    "\n",
    "# Plot projected device positions as line strip\n",
    "for go_pro in go_pro_proxy:\n",
    "    points = per_go_pro_reprojection[go_pro['name']]\n",
    "    rr.log(\n",
    "        f\"image/{go_pro['name']}/ego_device_translation_projection\",\n",
    "        rr.LineStrips2D(\n",
    "            points,\n",
    "            radii=[20],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Showing the rerun window\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c462f6-9b4c-4010-bcac-b799987a8cab",
   "metadata": {},
   "source": [
    "# Image undistortion\n",
    "\n",
    "Take Home Message:\n",
    "- You learned about how to:\n",
    "  - create a pinhole camera model to get an undistorted version of a given VRS image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b3c03-6ee7-41b8-b146-f139406eec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "from projectaria_tools.core import calibration\n",
    "from projectaria_tools.core.calibration import (\n",
    "    CameraCalibration,\n",
    "    distort_by_calibration,\n",
    ")\n",
    "\n",
    "from PIL import Image \n",
    "from IPython.display import display # to display images\n",
    "\n",
    "def get_image_stream_ids() -> List[StreamId]:\n",
    "    \"\"\"\n",
    "    Return the list of image stream ids\n",
    "    \"\"\"\n",
    "    stream_ids = vrs_data_provider.get_all_streams()\n",
    "    image_stream_ids = [\n",
    "        p\n",
    "        for p in stream_ids\n",
    "        if vrs_data_provider.get_label_from_stream_id(p).startswith(\n",
    "            \"camera-\"\n",
    "        ) and \"et\" not in vrs_data_provider.get_label_from_stream_id(p)\n",
    "    ]\n",
    "    return image_stream_ids\n",
    "    \n",
    "def get_camera_calibration(\n",
    "        stream_id: StreamId\n",
    "    ) -> CameraCalibration:\n",
    "    device_calibration = vrs_data_provider.get_device_calibration()\n",
    "    \n",
    "    stream_label = vrs_data_provider.get_label_from_stream_id(stream_id)\n",
    "    camera_calibration = device_calibration.get_camera_calib(stream_label)\n",
    "    return camera_calibration\n",
    "\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Aria - Image undistortion\")\n",
    "rec = rr.memory_recording()\n",
    "\n",
    "def pil_grid(images, max_horiz=np.iinfo(int).max):\n",
    "    n_images = len(images)\n",
    "    n_horiz = min(n_images, max_horiz)\n",
    "    h_sizes, v_sizes = [0] * n_horiz, [0] * (n_images // n_horiz)\n",
    "    for i, im in enumerate(images):\n",
    "        h, v = i % n_horiz, i // n_horiz\n",
    "        h_sizes[h] = max(h_sizes[h], im.size[0])\n",
    "        v_sizes[v] = max(v_sizes[v], im.size[1])\n",
    "    h_sizes, v_sizes = np.cumsum([0] + h_sizes), np.cumsum([0] + v_sizes)\n",
    "    im_grid = Image.new('RGB', (h_sizes[-1], v_sizes[-1]), color='white')\n",
    "    for i, im in enumerate(images):\n",
    "        im_grid.paste(im, (h_sizes[i % n_horiz], v_sizes[i // n_horiz]))\n",
    "    return im_grid\n",
    "\n",
    "images = []\n",
    "sample_count = 2\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for time_sample in tqdm(sample_timestamps):\n",
    "\n",
    "    for stream_id in get_image_stream_ids():\n",
    "\n",
    "        # Retrieving the image\n",
    "        image_tuple = vrs_data_provider.get_image_data_by_time_ns(stream_id, int(time_sample), time_domain, option)\n",
    "\n",
    "        #\n",
    "        # Camera undistortion\n",
    "        #  - We are creating a pinhole camera (target calibration)\n",
    "        #  - Then the undistortion will resample the camera ray from the original camera calibration and create the expected image\n",
    "        #\n",
    "        \n",
    "        # Retrieve the camera calibration attached to the Image (stream_id)\n",
    "        camera_calibration = get_camera_calibration(stream_id)\n",
    "\n",
    "        # Building the target calibration (Pinhole camera) to get the undistorted image\n",
    "        focal_lengths = camera_calibration.get_focal_lengths()\n",
    "        image_size = camera_calibration.get_image_size()\n",
    "        pinhole_calib = calibration.get_linear_camera_calibration(\n",
    "            image_size[0], image_size[1], focal_lengths[0]\n",
    "        )\n",
    "\n",
    "        # Compute the actual undistorted image (pixel sampling by using ray projection/reprojection)\n",
    "        undistorted_image = distort_by_calibration(\n",
    "            image_tuple[0].to_numpy_array(), pinhole_calib, camera_calibration\n",
    "        )\n",
    "\n",
    "        # Concat images in an array (show side by side Original - Undistorted images)\n",
    "        original_image_display = Image.fromarray(image_tuple[0].to_numpy_array())\n",
    "        undistorted_image_display = Image.fromarray(undistorted_image)\n",
    "        # resize for easy previewing\n",
    "        resampling_factor = 6 if stream_id == StreamId(\"214-1\") else 2\n",
    "        original_image_display = original_image_display.resize((image_size / resampling_factor).astype(int))\n",
    "        undistorted_image_display = undistorted_image_display.resize((image_size / resampling_factor).astype(int))\n",
    "        images.append(original_image_display)\n",
    "        images.append(undistorted_image_display)\n",
    "    \n",
    "concat = pil_grid(images, 6) # RGB, SLAM Left, SLAM Right\n",
    "display(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21031f1a-4095-4bd1-84b7-b944a2645734",
   "metadata": {},
   "source": [
    "## Bonus - Using Eye Gaze data to project on various image stream\n",
    "\n",
    "Take Home Message:\n",
    "- Your learned about:\n",
    "  - Using the `MpsDataProvider` to retrieve if an EyeGaze file is available and to retrieve EyeGaze data at a given timestamp\n",
    "  - That EyeGaze data is represented as a 3D ray with depth (showing the point of user focus)\n",
    "  - That EyeGaze ray is starting from CPF (Central Pupil Frame)\n",
    "  - How to use this EyeGaze data to reproject in any Aria Image Stream (RGB, SLAMs)\n",
    " \n",
    "Note that you don't need to use any 3D device pose, since EyeGaze is independent of the device pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dc472-0f3e-4324-b4be-0b8b37bf0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial on how to use EyeGaze data\n",
    "\n",
    "from projectaria_tools.core import mps\n",
    "from projectaria_tools.core.mps.utils import get_gaze_vector_reprojection\n",
    "\n",
    "# Init rerun api\n",
    "rr.init(\"Eye Gaze - CPF - Image Reprojection\")\n",
    "rec = rr.memory_recording()\n",
    "# Aria coordinate system sets X down, Z in front, Y Left\n",
    "rr.log(\"device\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n",
    "\n",
    "## Configure the MpsDataProvider (interface used to retrieve EyeGaze data)\n",
    "mps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\n",
    "mps_data_paths = mps_data_paths_provider.get_data_paths()\n",
    "mps_data_provider = mps.MpsDataProvider(mps_data_paths)\n",
    "\n",
    "assert mps_data_provider.has_personalized_eyegaze(), \"The sequence does not have Eye Gaze data\"\n",
    "\n",
    "# Plot the Aria Glasses in 3D to give context, on where does Eye Gaze Tracking origin is.\n",
    "\n",
    "## Plot CPF (Central Pupil Frame coordinate system)\n",
    "T_device_CPF = device_calibration.get_transform_device_cpf()\n",
    "log_pose(T_device_CPF, \"device/CPF_CentralPupilFrame\", timeless=True)\n",
    "## Plot Project Aria Glasses outline (as lines)\n",
    "log_aria_glasses(device_calibration, \"device/glasses_outline\")\n",
    "\n",
    "# Retrieve the RGB camera calibration (required for projecting the Eye Gaze 3D point to the image plane)\n",
    "rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
    "slam_left_camera_calibration = device_calibration.get_camera_calib(slam_left_stream_label)\n",
    "slam_right_camera_calibration = device_calibration.get_camera_calib(slam_right_stream_label)\n",
    "\n",
    "sample_count = 20\n",
    "sample_timestamps = np.linspace(start_time, end_time, sample_count)\n",
    "for sample in tqdm(sample_timestamps):\n",
    "\n",
    "    # Retrieving the RGB image\n",
    "    image_tuple_rgb = vrs_data_provider.get_image_data_by_time_ns(rgb_stream_id, int(sample), time_domain, option)\n",
    "    timestamp = image_tuple_rgb[1].capture_timestamp_ns\n",
    "    \n",
    "    # Log timestamp as:\n",
    "    # - device_time (so you can see the effective time between two frames)\n",
    "    # - timestamp (so you can see the real VRS timestamp as INT value in the Rerun Timeline dropdown)\n",
    "    rr.set_time_nanos(\"device_time\", timestamp)\n",
    "    rr.set_time_sequence(\"timestamp\", timestamp)\n",
    "\n",
    "    log_image(image_tuple_rgb[0].to_numpy_array(), f\"device/{rgb_stream_label}\")\n",
    "    \n",
    "    # Retrieving the SLAM images\n",
    "    image_tuple_slam_left = vrs_data_provider.get_image_data_by_time_ns(slam_left_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_left[0].to_numpy_array(), f\"device/{slam_left_stream_label}\")\n",
    "\n",
    "    image_tuple_slam_right = vrs_data_provider.get_image_data_by_time_ns(slam_right_stream_id, int(sample), time_domain, option)\n",
    "    log_image(image_tuple_slam_right[0].to_numpy_array(), f\"device/{slam_right_stream_label}\")\n",
    "\n",
    "    ##\n",
    "    # Eye Gaze data\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    ##\n",
    "\n",
    "    # 1. Retrieve the eye_gaze data vector for a given timestamp\n",
    "    eye_gaze = mps_data_provider.get_personalized_eyegaze(timestamp)\n",
    "\n",
    "    # 2. Compute the corresponding 3D vector and retrieve its depth\n",
    "    # Here is how to retrieve the depth of the EyeGaze vector\n",
    "    # depth_m = eye_gaze.depth or 1.0\n",
    "    # But here for display we are using a proxy of 30cm, so you can better see things in context of each other\n",
    "    depth_m = 0.1\n",
    "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(\n",
    "        eye_gaze.yaw, eye_gaze.pitch, depth_m\n",
    "    )\n",
    "    gaze_vector_in_cpf = np.nan_to_num(gaze_vector_in_cpf)\n",
    "    # Move EyeGaze vector to CPF coordinate system for visualization\n",
    "    rr.log(\n",
    "        \"device/eye-gaze\",\n",
    "        rr.Arrows3D(\n",
    "            origins=[T_device_CPF @ [0, 0, 0]],\n",
    "            vectors=[T_device_CPF @ gaze_vector_in_cpf],\n",
    "            colors=[[255, 0, 255]],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 3. Reproject the eyegaze vector at Depth X on a given image (using Calibration data)\n",
    "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
    "    depth_m = eye_gaze.depth or 1.0\n",
    "\n",
    "    for stream_label in [rgb_stream_label, slam_left_stream_label, slam_right_stream_label]:\n",
    "        if stream_label is rgb_stream_label:\n",
    "            camera_calibration = rgb_camera_calibration\n",
    "        elif stream_label is slam_left_stream_label:\n",
    "            camera_calibration = slam_left_camera_calibration\n",
    "        elif stream_label is slam_right_stream_label:\n",
    "            camera_calibration = slam_right_camera_calibration\n",
    "        else:\n",
    "            camera_calibration = None\n",
    "\n",
    "        gaze_projection = get_gaze_vector_reprojection(\n",
    "            eye_gaze,\n",
    "            stream_label,\n",
    "            device_calibration,\n",
    "            camera_calibration,\n",
    "            depth_m,\n",
    "        )\n",
    "        if gaze_projection is not None:\n",
    "            rr.log(\n",
    "                f\"device/{stream_label}/eye-gaze_projection\",\n",
    "                rr.Points2D(gaze_projection, radii=20),\n",
    "            )\n",
    "\n",
    "\n",
    "# Showing the rerun window\n",
    "rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
